<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   

  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/celeste.min.css">

<link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
<!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
<link rel="icon" type="image/png" sizes="152x152" href="/assets/robot_smaller_152x152.png">
<link rel="icon" type="image/x-icon" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/x-icon" sizes="32x32" href="/assets/robot_smaller_32x32.png">
<link rel="icon" type="image/png" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/robot_smaller_32x32.png">


   <title>NLPwShiyi - Natural Language Processing with Shiyi</title>  
</head>
<body>
  <!-- Latest compiled and minified CSS -->

<nav id="navbar" class="navigation" role="navigation">
  <input id="toggle1" type="checkbox" />
  <label class="hamburger1" for="toggle1">
    <div class="top"></div>
    <div class="meat"></div>
    <div class="bottom"></div>
  </label>

  <nav class="topnav mx-auto" id="myTopnav">
    <div class="dropdown">
      <button class="dropbtn">Comp Ling
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/1a-phil-of-mind">Philosophy of Mind</a> -->
        <a href="/modules/1b-info-theory">Information Theory</a>
        <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
        <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
        <a href="/modules/1e-mutual-info">Mutual Information</a>
        <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
        <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
        <a href="/modules/1h-semantics">Logic and Problem Solving</a>
        <!-- <a href="/modules/1i-cryptanalysis">Cryptography</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >DL / NLP
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/2a-mdn-nlp">Modern NLP</a> -->
        <a href="/modules/2b-markov-processes">Markov Processes</a>
        <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
        <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
        <a href="/modules/2f-loss-functions">Stochastic GD</a>
        <a href="/modules/2c-word2vec">Word2Vec</a>
        <a href="/modules/2g-batchnorm">Batchnorm</a>
        <a href="/modules/2j-perplexity">Perplexity</a>
        <a href="/modules/2h-dropout">Dropout</a>
        <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        <a href="/modules/2k-VAE">Variational Autoencoders</a>
        <a href="/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
      </div>
    </div>
    <a href="/" class="active">Intro </a>
    <div class="dropdown">
      <button class="dropbtn" >SOTA
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">

        <a href="/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a>
        <a href="/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a>
        <a href="/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a>
        <a href="/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a>
        <a href="/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a>
        
        <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >Hands-on
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
        <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
        <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
        <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a>
        <a href="/modules/4e-c4fe-tbip">Text-based Ideal Points Model</a>
        <!-- <a href="/modules/4f-server-build">WebServer Build Example</a> -->
        <a href="/modules/4g-etl-job">Serverless ETL Example</a>
        <a href="/modules/4h-ocr-data-aug">OCR Text Augmentation</a>
        <a href="/modules/4i-neo4j-gql">Neo4j GQL Example</a>
        <!-- <a href="/modules/4j-amr-parser">AMR Parser Example</a> -->


        

      </div>
    </div>
  </nav>
</nav>
  
  
<!-- Content appended here -->
  <!-- {{if hascode}} {{insert copy_paste.html}} {{end}} --><div class="franklin-content"><h2 id="measuring_political_subjectivity_with_variational_encoding_methods"><a href="#measuring_political_subjectivity_with_variational_encoding_methods" class="header-anchor">Measuring Political Subjectivity with Variational Encoding Methods </a></h2>
<div class="franklin-toc"><ol><li><a href="#measuring_political_subjectivity_with_variational_encoding_methods">Measuring Political Subjectivity with Variational Encoding Methods </a></li><li><a href="#the_motivation_behind_this_project">The Motivation behind This Project </a></li><li><a href="#model_implementation_with_numpyro">Model Implementation with NumPyro</a></li></ol></div>
<p>In socio-politics, quantified approaches and modeling techniques are applied in supporting and facilitating political analyses. Individuals, parties, committees and other political entities come together and try to push forward campaigns in hope to receive appropriate patrionization and support for their political agenda. </p>
<p>The Political Action Committees &#40;PACs or Super PACs&#41; amass funding resources that could benefit the elections. These type of fundings could be from other individuals, or political entities. For the sole of purpose of understanding what the processes of fund raising activities like these really are, this part of the project explores the 2021-2022 PACs financial data.</p>
<p>This part of the project will first present the receipts, disbursements, and other expenditures in terms of propagating political actions in visualization format grounded in states; for example, how many different political action committees there are by US states. </p>
<p>This part of the project will also break down all the candidates of 2022 their basic information as mentioned above including their basic demographics, political party affiliation, election cycle, and incumbency.</p>
<p>All info is retrievable through the Federal Election Commission&#39;s directory. This project seeks to conduct the research with full transparency and abide to relevant code of conduct.</p>
<h2 id="the_motivation_behind_this_project"><a href="#the_motivation_behind_this_project" class="header-anchor">The Motivation behind This Project </a></h2>
<p>Measuring political sentiment and polarization is a common practice in the realm of social science research. However, it may also be applicable to solving business problems, like providing more information about a certain candidate to voters to fill the information gap and facilitate voting processes. </p>
<p>This project tries to help someone who is interested in voting activities understand the political leaning of a candidate for federal elections. </p>
<p>In this blog, the structure and construct of the model will be explained. Please check out this <a href="https://github.com/shiyis/c4fe-tbip">repo</a> for a more comprehensive demo of the project and other complementary analysis. </p>
<p>draws inspiration from website like <a href="https://www.opensecrets.org/">OpenSecrets</a> and <a href="https://arxiv.org/abs/2005.04232">this paper</a>, where it strives to uncover information of a politician&#39;s agenda and activities &#40;campaign-related or financial&#41;.</p>
<p>helps the general population who is interested in partaking in political activities understand a politician &#40;or anyone who authors political content&#41;&#39;s leaning/stance by extracting crucial information from relevant political text. </p>
<p>Website like <a href="https://www.opensecrets.org/">OpenSecrets</a> provides valuable statistics and educational information to start. This project tries to top it off by retrieving organic information &#40;Tweets&#41; of said candidates and conduct analysis accordingly. </p>
<h2 id="model_implementation_with_numpyro"><a href="#model_implementation_with_numpyro" class="header-anchor">Model Implementation with NumPyro</a></h2>
<pre><code class="language-python">&#37;&#37;capture
&#37;pip install numpyro&#61;&#61;0.10.1
&#37;pip install optax</code></pre>
<pre><code class="language-python">from scipy import sparse
import jax
import jax.numpy as jnp
import numpy as np

dataPath &#61; &quot;tbip/data/senate-speeches-114/clean/&quot;

# Load data
author_indices &#61; jax.device_put&#40;
    jnp.load&#40;dataPath &#43; &quot;author_indices.npy&quot;&#41;, jax.devices&#40;&quot;gpu&quot;&#41;&#91;0&#93;
&#41;

counts &#61; sparse.load_npz&#40;dataPath &#43; &quot;counts.npz&quot;&#41;

with open&#40;dataPath &#43; &quot;vocabulary.txt&quot;, &quot;r&quot;&#41; as f:
    vocabulary &#61; f.readlines&#40;&#41;

with open&#40;dataPath &#43; &quot;author_map.txt&quot;, &quot;r&quot;&#41; as f:
    author_map &#61; f.readlines&#40;&#41;

author_map &#61; np.array&#40;author_map&#41;

num_authors &#61; int&#40;author_indices.max&#40;&#41; &#43; 1&#41;
num_documents, num_words &#61; counts.shape</code></pre>
<pre><code class="language-python">pre_initialize_parameters &#61; True</code></pre>
<pre><code class="language-python"># Fit NMF to be used as initialization for TBIP
from sklearn.decomposition import NMF

if pre_initialize_parameters:
    nmf_model &#61; NMF&#40;
        n_components&#61;num_topics, init&#61;&quot;random&quot;, random_state&#61;0, max_iter&#61;500
    &#41;
    # Define initialization arrays
    initial_document_loc &#61; jnp.log&#40;
        jnp.array&#40;np.float32&#40;nmf_model.fit_transform&#40;counts&#41; &#43; 1e-2&#41;&#41;
    &#41;
    initial_objective_topic_loc &#61; jnp.log&#40;
        jnp.array&#40;np.float32&#40;nmf_model.components_ &#43; 1e-2&#41;&#41;
    &#41;
else:
    rng1, rng2 &#61; random.split&#40;rng_seed, 2&#41;
    initial_document_loc &#61; random.normal&#40;rng1, 
                                        shape&#61;&#40;num_documents, num_topics&#41;&#41;
    initial_objective_topic_loc &#61; random.normal&#40;rng2, 
                                        shape&#61;&#40;num_topics, num_words&#41;&#41;</code></pre>
<pre><code class="language-python"># Fit NMF to be used as initialization for TBIP
from sklearn.decomposition import NMF

if pre_initialize_parameters:
    nmf_model &#61; NMF&#40;
        n_components&#61;num_topics, init&#61;&quot;random&quot;, random_state&#61;0, max_iter&#61;500
    &#41;
    # Define initialization arrays
    initial_document_loc &#61; jnp.log&#40;
        jnp.array&#40;np.float32&#40;nmf_model.fit_transform&#40;counts&#41; &#43; 1e-2&#41;&#41;
    &#41;
    initial_objective_topic_loc &#61; jnp.log&#40;
        jnp.array&#40;np.float32&#40;nmf_model.components_ &#43; 1e-2&#41;&#41;
    &#41;
else:
    rng1, rng2 &#61; random.split&#40;rng_seed, 2&#41;
    initial_document_loc &#61; random.normal&#40;rng1, shape&#61;&#40;num_documents, num_topics&#41;&#41;
    initial_objective_topic_loc &#61; random.normal&#40;rng2, shape&#61;&#40;num_topics, num_words&#41;&#41;</code></pre>
<p>The results are inferred using variational inference with reparameterization gradients. </p>
<p>It is intractable to evaluate the posterior distribution, so we approximate the posterior with a distribution. How do we set the values? We want to minimize the KL-Divergence between and the posterior, which is equivalent to maximizing the ELBO:</p>
<p>Sure, here is the LaTeX representation of the Evidence Lower Bound &#40;ELBO&#41; and the Kullback-Leibler &#40;KL&#41; divergence:</p>
\[
\text{ELBO} = \mathbb{E}_{q(z|x)}[\log p(x, z) - \log q(z|x)]
\]
\[
\text{KL}(q(z|x)||p(z)) = \mathbb{E}_{q(z|x)}[\log q(z|x) - \log p(z)]
\]
<p>In these equations:</p>
<ul>
<li><p>\(q(z|x)\) represents the approximate posterior distribution over latent variables \(z\) given input data \(x\) </p>
</li>
<li><p>\(p(x, z)\) is the joint distribution of the observed data \(x\) and the latent variables \(z\) - it&#39;s essentially the likelihood of generating the observed documents given the latent variables - it quantifies how likely it is to see a particular set of documents along with their associated latent representations.</p>
</li>
<li><p>\(p(z)\) is the prior distribution over latent variables - in the context of document clustering, it can represent the prior distribution of topics over documents, capturing assumptions about the distribution of topics in the dataset.</p>
</li>
<li><p>The expectation \(\mathbb{E}_{q(z|x)}[\cdot]\) is taken with respect to the approximate posterior \(q(z|x)\).</p>
</li>
<li><p>The ELBO is the lower bound on the log-likelihood of the observed data \(x\), and maximizing it is equivalent to minimizing the KL divergence between the approximate posterior \(q(z|x)\) and the true prior \(p(z)\).</p>
</li>
</ul>
<p>We set the variational family to be the mean-field family, meaning the latent variables factorize over documents, topics ,and authors :</p>
\[ q_\phi(\theta, \beta, \eta, x) = \prod_{d,k,s} q(\theta_d)q(\beta_k)q(\eta_k)q(x_s) \]
<p>We use lognormal factors for the positive variables and Gaussian factors for the real variables:</p>
\[q(\theta_d) = \text{LogNormal}_K(\mu_{\theta_d}\sigma^2_{\theta_d})\]
\[q(\beta_k) = \text{LogNormal}_V(\mu_{\beta_k}, \sigma^2_{\beta_k})\]
\[q(\eta_k) = \mathcal{N}_V(\mu_{\eta_k}, \sigma^2_{\eta_k})\]
\[q(x_s) = \mathcal{N}(\mu_{x_s}, \sigma^2_{x_s}).\]
<p>Thus, our goal is to maximize the ELBO with respect to  </p>
\[\phi = \{\mu_\theta, \sigma_\theta, \mu_\beta, \sigma_\beta,\mu_\eta, \sigma_\eta, \mu_x, \sigma_x\}\]
<pre><code class="language-python">In the cell below, we define the model and the variational family &#40;guide&#41;.

from numpyro import plate, sample, param
import numpyro.distributions as dist
from numpyro.distributions import constraints

# Define the model and variational family


class TBIP:
    def __init__&#40;self, N, D, K, V, batch_size, init_mu_theta&#61;None, init_mu_beta&#61;None&#41;:
        self.N &#61; N  # number of people
        self.D &#61; D  # number of documents
        self.K &#61; K  # number of topics
        self.V &#61; V  # number of words in vocabulary
        self.batch_size &#61; batch_size  # number of documents in a batch

        if init_mu_theta is None:
            init_mu_theta &#61; jnp.zeros&#40;&#91;D, K&#93;&#41;
        else:
            self.init_mu_theta &#61; init_mu_theta

        if init_mu_beta is None:
            init_mu_beta &#61; jnp.zeros&#40;&#91;K, V&#93;&#41;
        else:
            self.init_mu_beta &#61; init_mu_beta

    def model&#40;self, Y_batch, d_batch, i_batch&#41;:
        with plate&#40;&quot;i&quot;, self.N&#41;:
            # Sample the per-unit latent variables &#40;ideal points&#41;
            x &#61; sample&#40;&quot;x&quot;, dist.Normal&#40;&#41;&#41;

        with plate&#40;&quot;k&quot;, size&#61;self.K, dim&#61;-2&#41;:
            with plate&#40;&quot;k_v&quot;, size&#61;self.V, dim&#61;-1&#41;:
                beta &#61; sample&#40;&quot;beta&quot;, dist.Gamma&#40;0.3, 0.3&#41;&#41;
                eta &#61; sample&#40;&quot;eta&quot;, dist.Normal&#40;&#41;&#41;

        with plate&#40;&quot;d&quot;, size&#61;self.D, subsample_size&#61;self.batch_size, dim&#61;-2&#41;:
            with plate&#40;&quot;d_k&quot;, size&#61;self.K, dim&#61;-1&#41;:
                # Sample document-level latent variables &#40;topic intensities&#41;
                theta &#61; sample&#40;&quot;theta&quot;, dist.Gamma&#40;0.3, 0.3&#41;&#41;

            # Compute Poisson rates for each word
            P &#61; jnp.sum&#40;
                jnp.expand_dims&#40;theta, 2&#41;
                * jnp.expand_dims&#40;beta, 0&#41;
                * jnp.exp&#40;
                    jnp.expand_dims&#40;x&#91;i_batch&#93;, &#40;1, 2&#41;&#41; * 
                    jnp.expand_dims&#40;eta, 0&#41;
                &#41;,
                1,
            &#41;

        with plate&#40;&quot;v&quot;, size&#61;self.V, dim&#61;-1&#41;:
            # Sample observed words
            sample&#40;&quot;Y_batch&quot;, dist.Poisson&#40;P&#41;, obs&#61;Y_batch&#41;

    def guide&#40;self, Y_batch, d_batch, i_batch&#41;:
        # This defines variational family. Notice that each of the latent variables
        # defined in the sample statements in the model above has a corresponding
        # sample statement in the guide. The guide is responsible for providing
        # variational parameters for each of these latent variables.

        # Also notice it is required that model and the guide have the same call.

        mu_x &#61; param&#40;
            &quot;mu_x&quot;, init_value&#61;-1 &#43; 2 * random.uniform&#40;random.PRNGKey&#40;1&#41;, &#40;self.N,&#41;&#41;
        &#41;
        sigma_x &#61; param&#40;
            &quot;sigma_y&quot;, init_value&#61;jnp.ones&#40;&#91;self.N&#93;&#41;, constraint&#61;constraints.positive
        &#41;

        mu_eta &#61; param&#40;
            &quot;mu_eta&quot;, init_value&#61;random.normal&#40;random.PRNGKey&#40;2&#41;, &#40;self.K, self.V&#41;&#41;
        &#41;
        sigma_eta &#61; param&#40;
            &quot;sigma_eta&quot;,
            init_value&#61;jnp.ones&#40;&#91;self.K, self.V&#93;&#41;,
            constraint&#61;constraints.positive,
        &#41;

        mu_theta &#61; param&#40;&quot;mu_theta&quot;, init_value&#61;self.init_mu_theta&#41;
        sigma_theta &#61; param&#40;
            &quot;sigma_theta&quot;,
            init_value&#61;jnp.ones&#40;&#91;self.D, self.K&#93;&#41;,
            constraint&#61;constraints.positive,
        &#41;

        mu_beta &#61; param&#40;&quot;mu_beta&quot;, init_value&#61;self.init_mu_beta&#41;
        sigma_beta &#61; param&#40;
            &quot;sigma_beta&quot;,
            init_value&#61;jnp.ones&#40;&#91;self.K, self.V&#93;&#41;,
            constraint&#61;constraints.positive,
        &#41;

        with plate&#40;&quot;i&quot;, self.N&#41;:
            sample&#40;&quot;x&quot;, dist.Normal&#40;mu_x, sigma_x&#41;&#41;

        with plate&#40;&quot;k&quot;, size&#61;self.K, dim&#61;-2&#41;:
            with plate&#40;&quot;k_v&quot;, size&#61;self.V, dim&#61;-1&#41;:
                sample&#40;&quot;beta&quot;, dist.LogNormal&#40;mu_beta, sigma_beta&#41;&#41;
                sample&#40;&quot;eta&quot;, dist.Normal&#40;mu_eta, sigma_eta&#41;&#41;

        with plate&#40;&quot;d&quot;, size&#61;self.D, subsample_size&#61;self.batch_size, dim&#61;-2&#41;:
            with plate&#40;&quot;d_k&quot;, size&#61;self.K, dim&#61;-1&#41;:
                sample&#40;&quot;theta&quot;, dist.LogNormal&#40;mu_theta&#91;d_batch&#93;, sigma_theta&#91;d_batch&#93;&#41;&#41;

    def get_batch&#40;self, rng, Y, author_indices&#41;:
        # Helper functions to obtain a batch of data, convert from scipy.sparse
        # to jax.numpy.array and move to gpu

        D_batch &#61; random.choice&#40;rng, jnp.arange&#40;self.D&#41;, shape&#61;&#40;self.batch_size,&#41;&#41;
        Y_batch &#61; jax.device_put&#40;jnp.array&#40;Y&#91;D_batch&#93;.toarray&#40;&#41;&#41;, jax.devices&#40;&quot;gpu&quot;&#41;&#91;0&#93;&#41;
        D_batch &#61; jax.device_put&#40;D_batch, jax.devices&#40;&quot;gpu&quot;&#41;&#91;0&#93;&#41;
        I_batch &#61; author_indices&#91;D_batch&#93;
        return Y_batch, I_batch, D_batch</code></pre>
<pre><code class="language-python"># Initialize the model
from optax import adam, exponential_decay
from numpyro.infer import SVI, TraceMeanField_ELBO
from jax import jit

num_steps &#61; 50000
batch_size &#61; 512  # Large batches are recommended
learning_rate &#61; 0.01
decay_rate &#61; 0.01

tbip &#61; TBIP&#40;
    N&#61;num_authors,
    D&#61;num_documents,
    K&#61;num_topics,
    V&#61;num_words,
    batch_size&#61;batch_size,
    init_mu_theta&#61;initial_document_loc,
    init_mu_beta&#61;initial_objective_topic_loc,
&#41;

svi_batch &#61; SVI&#40;
    model&#61;tbip.model,
    guide&#61;tbip.guide,
    optim&#61;adam&#40;exponential_decay&#40;learning_rate, num_steps, decay_rate&#41;&#41;,
    loss&#61;TraceMeanField_ELBO&#40;&#41;,
&#41;

# Compile update function for faster training
svi_batch_update &#61; jit&#40;svi_batch.update&#41;

# Get initial batch. This informs the dimension of arrays and ensures they are
# consistent with dimensions &#40;N, D, K, V&#41; defined above.
Y_batch, I_batch, D_batch &#61; tbip.get_batch&#40;random.PRNGKey&#40;1&#41;, counts, author_indices&#41;

# Initialize the parameters using initial batch
svi_state &#61; svi_batch.init&#40;
    random.PRNGKey&#40;0&#41;, Y_batch&#61;Y_batch, d_batch&#61;D_batch, i_batch&#61;I_batch
&#41;</code></pre>
<pre><code class="language-python"># @title Run this cell to create helper function for printing topics


def get_topics&#40;
    neutral_mean, negative_mean, positive_mean, vocabulary, print_to_terminal&#61;True
&#41;:
    num_topics, num_words &#61; neutral_mean.shape
    words_per_topic &#61; 10
    top_neutral_words &#61; np.argsort&#40;-neutral_mean, axis&#61;1&#41;
    top_negative_words &#61; np.argsort&#40;-negative_mean, axis&#61;1&#41;
    top_positive_words &#61; np.argsort&#40;-positive_mean, axis&#61;1&#41;
    topic_strings &#61; &#91;&#93;
    for topic_idx in range&#40;num_topics&#41;:
        neutral_start_string &#61; &quot;Neutral  &#123;&#125;:&quot;.format&#40;topic_idx&#41;
        neutral_row &#61; &#91;
            vocabulary&#91;word&#93; for word in top_neutral_words&#91;topic_idx, :words_per_topic&#93;
        &#93;
        neutral_row_string &#61; &quot;, &quot;.join&#40;neutral_row&#41;
        neutral_string &#61; &quot; &quot;.join&#40;&#91;neutral_start_string, neutral_row_string&#93;&#41;

        positive_start_string &#61; &quot;Positive &#123;&#125;:&quot;.format&#40;topic_idx&#41;
        positive_row &#61; &#91;
            vocabulary&#91;word&#93; for word in top_positive_words&#91;topic_idx, :words_per_topic&#93;
        &#93;
        positive_row_string &#61; &quot;, &quot;.join&#40;positive_row&#41;
        positive_string &#61; &quot; &quot;.join&#40;&#91;positive_start_string, positive_row_string&#93;&#41;

        negative_start_string &#61; &quot;Negative &#123;&#125;:&quot;.format&#40;topic_idx&#41;
        negative_row &#61; &#91;
            vocabulary&#91;word&#93; for word in top_negative_words&#91;topic_idx, :words_per_topic&#93;
        &#93;
        negative_row_string &#61; &quot;, &quot;.join&#40;negative_row&#41;
        negative_string &#61; &quot; &quot;.join&#40;&#91;negative_start_string, negative_row_string&#93;&#41;

        if print_to_terminal:
            topic_strings.append&#40;negative_string&#41;
            topic_strings.append&#40;neutral_string&#41;
            topic_strings.append&#40;positive_string&#41;
            topic_strings.append&#40;&quot;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&#61;&quot;&#41;
        else:
            topic_strings.append&#40;
                &quot;  \n&quot;.join&#40;&#91;negative_string, neutral_string, positive_string&#93;&#41;
            &#41;

    if print_to_terminal:
        all_topics &#61; &quot;&#123;&#125;\n&quot;.format&#40;np.array&#40;topic_strings&#41;&#41;
    else:
        all_topics &#61; np.array&#40;topic_strings&#41;
    return all_topics</code></pre>
<pre><code class="language-python"># Run SVI
from tqdm import tqdm
import pandas as pd

print_steps &#61; 100
print_intermediate_results &#61; False

rngs &#61; random.split&#40;random.PRNGKey&#40;2&#41;, num_steps&#41;
losses &#61; &#91;&#93;
pbar &#61; tqdm&#40;range&#40;num_steps&#41;&#41;


for step in pbar:
    Y_batch, I_batch, D_batch &#61; tbip.get_batch&#40;rngs&#91;step&#93;, counts, author_indices&#41;
    svi_state, loss &#61; svi_batch_update&#40;
        svi_state, Y_batch&#61;Y_batch, d_batch&#61;D_batch, i_batch&#61;I_batch
    &#41;

    loss &#61; loss / counts.shape&#91;0&#93;
    losses.append&#40;loss&#41;
    if step &#37; print_steps &#61;&#61; 0 or step &#61;&#61; num_steps - 1:
        pbar.set_description&#40;
            &quot;Init loss: &quot;
            &#43; &quot;&#123;:10.4f&#125;&quot;.format&#40;jnp.array&#40;losses&#91;0&#93;&#41;&#41;
            &#43; f&quot;; Avg loss &#40;last &#123;print_steps&#125; iter&#41;: &quot;
            &#43; &quot;&#123;:10.4f&#125;&quot;.format&#40;jnp.array&#40;losses&#91;-100:&#93;&#41;.mean&#40;&#41;&#41;
        &#41;

    if &#40;step &#43; 1&#41; &#37; 2500 &#61;&#61; 0 or step &#61;&#61; num_steps - 1:
        # Save intermediate results
        estimated_params &#61; svi_batch.get_params&#40;svi_state&#41;

        neutral_mean &#61; &#40;
            estimated_params&#91;&quot;mu_beta&quot;&#93; &#43; estimated_params&#91;&quot;sigma_beta&quot;&#93; ** 2 / 2
        &#41;

        positive_mean &#61; &#40;
            estimated_params&#91;&quot;mu_beta&quot;&#93;
            &#43; estimated_params&#91;&quot;mu_eta&quot;&#93;
            &#43; &#40;estimated_params&#91;&quot;sigma_beta&quot;&#93; ** 2 &#43; estimated_params&#91;&quot;sigma_eta&quot;&#93; ** 2&#41;
            / 2
        &#41;

        negative_mean &#61; &#40;
            estimated_params&#91;&quot;mu_beta&quot;&#93;
            - estimated_params&#91;&quot;mu_eta&quot;&#93;
            &#43; &#40;estimated_params&#91;&quot;sigma_beta&quot;&#93; ** 2 &#43; estimated_params&#91;&quot;sigma_eta&quot;&#93; ** 2&#41;
            / 2
        &#41;

        np.save&#40;&quot;neutral_topic_mean.npy&quot;, neutral_mean&#41;
        np.save&#40;&quot;negative_topic_mean.npy&quot;, positive_mean&#41;
        np.save&#40;&quot;positive_topic_mean.npy&quot;, negative_mean&#41;

        topics &#61; get_topics&#40;neutral_mean, positive_mean, negative_mean, vocabulary&#41;

        with open&#40;&quot;topics.txt&quot;, &quot;w&quot;&#41; as f:
            print&#40;topics, file&#61;f&#41;

        authors &#61; pd.DataFrame&#40;
            &#123;&quot;name&quot;: author_map, &quot;ideal_point&quot;: np.array&#40;estimated_params&#91;&quot;mu_x&quot;&#93;&#41;&#125;
        &#41;
        authors.to_csv&#40;&quot;authors.csv&quot;&#41;

        if print_intermediate_results:
            print&#40;f&quot;Results after &#123;step&#125; steps.&quot;&#41;
            print&#40;topics&#41;
            sorted_authors &#61; &quot;Authors sorted by their ideal points: &quot; &#43; &quot;,&quot;.join&#40;
                list&#40;authors.sort_values&#40;&quot;ideal_point&quot;&#41;&#91;&quot;name&quot;&#93;&#41;
            &#41;
            print&#40;sorted_authors.replace&#40;&quot;\n&quot;, &quot; &quot;&#41;&#41;</code></pre>
<pre><code class="language-python">import os
import matplotlib.pyplot as plt
import seaborn as sns

neutral_topic_mean &#61; np.load&#40;&quot;neutral_topic_mean.npy&quot;&#41;
negative_topic_mean &#61; np.load&#40;&quot;negative_topic_mean.npy&quot;&#41;
positive_topic_mean &#61; np.load&#40;&quot;positive_topic_mean.npy&quot;&#41;
authors &#61; pd.read_csv&#40;&quot;authors.csv&quot;&#41;
authors&#91;&quot;name&quot;&#93; &#61; authors&#91;&quot;name&quot;&#93;.str.replace&#40;&quot;\n&quot;, &quot;&quot;&#41;</code></pre>
<pre><code class="language-python">selected_authors &#61; np.array&#40;
    &#91;
        &quot;Dean Heller &#40;R&#41;&quot;,
        &quot;Bernard Sanders &#40;I&#41;&quot;,
        &quot;Elizabeth Warren &#40;D&#41;&quot;,
        &quot;Charles Schumer &#40;D&#41;&quot;,
        &quot;Susan Collins &#40;R&#41;&quot;,
        &quot;Marco Rubio &#40;R&#41;&quot;,
        &quot;John Mccain &#40;R&#41;&quot;,
        &quot;Ted Cruz &#40;R&#41;&quot;,
    &#93;
&#41;

sns.set&#40;style&#61;&quot;whitegrid&quot;&#41;
fig &#61; plt.figure&#40;figsize&#61;&#40;12, 1&#41;&#41;
ax &#61; plt.axes&#40;&#91;0, 0, 1, 1&#93;, frameon&#61;False&#41;
for index in range&#40;authors.shape&#91;0&#93;&#41;:
    ax.scatter&#40;authors&#91;&quot;ideal_point&quot;&#93;&#91;index&#93;, 0, c&#61;&quot;black&quot;, s&#61;20&#41;
    if authors&#91;&quot;name&quot;&#93;&#91;index&#93; in selected_authors:
        ax.annotate&#40;
            author_map&#91;index&#93;,
            xy&#61;&#40;authors&#91;&quot;ideal_point&quot;&#93;&#91;index&#93;, 0.0&#41;,
            xytext&#61;&#40;authors&#91;&quot;ideal_point&quot;&#93;&#91;index&#93;, 0&#41;,
            rotation&#61;30,
            size&#61;14,
        &#41;
ax.set_yticks&#40;&#91;&#93;&#41;
plt.show&#40;&#41;</code></pre>
<pre><code class="language-python">from numpyro.infer.autoguide import AutoNormal


def create_svi_object&#40;guide&#41;:
    svi_object &#61; SVI&#40;
        model&#61;tbip.model,
        guide&#61;guide,
        optim&#61;adam&#40;exponential_decay&#40;learning_rate, num_steps, decay_rate&#41;&#41;,
        loss&#61;TraceMeanField_ELBO&#40;&#41;,
    &#41;

    Y_batch, I_batch, D_batch &#61; tbip.get_batch&#40;
        random.PRNGKey&#40;1&#41;, counts, author_indices
    &#41;

    svi_state &#61; svi_batch.init&#40;
        random.PRNGKey&#40;0&#41;, Y_batch&#61;Y_batch, d_batch&#61;D_batch, i_batch&#61;I_batch
    &#41;

    return svi_state


# This state uses the guide defined manually above
svi_state_manualguide &#61; create_svi_object&#40;guide&#61;tbip.guide&#41;

# Now let&#39;s create this object but using AutoNormal guide. We just need to ensure that
# parameters are initialized as above.
autoguide &#61; AutoNormal&#40;
    model&#61;tbip.model,
    init_loc_fn&#61;&#123;&quot;beta&quot;: initial_objective_topic_loc, &quot;theta&quot;: initial_document_loc&#125;,
&#41;
svi_state_autoguide &#61; create_svi_object&#40;guide&#61;autoguide&#41;


# Assert that the keys in the optimizer states are identical
assert svi_state_manualguide&#91;0&#93;&#91;1&#93;&#91;0&#93;.keys&#40;&#41; &#61;&#61; svi_state_autoguide&#91;0&#93;&#91;1&#93;&#91;0&#93;.keys&#40;&#41;

# Assert that the values in the optimizer states are identical
for key in svi_state_manualguide&#91;0&#93;&#91;1&#93;&#91;0&#93;.keys&#40;&#41;:
    assert jnp.all&#40;
        svi_state_manualguide&#91;0&#93;&#91;1&#93;&#91;0&#93;&#91;key&#93; &#61;&#61; svi_state_autoguide&#91;0&#93;&#91;1&#93;&#91;0&#93;&#91;key&#93;&#41;</code></pre>
<div class="page-foot">
    <div class="copyright">
      <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo" src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
       ©️ Last modified: May 09, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    </div>
  </div>
  </div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
