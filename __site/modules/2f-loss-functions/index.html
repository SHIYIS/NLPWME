<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   

  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/celeste.min.css">

<link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
<!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
<link rel="icon" type="image/png" sizes="152x152" href="/assets/robot_smaller_152x152.png">
<link rel="icon" type="image/x-icon" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/x-icon" sizes="32x32" href="/assets/robot_smaller_32x32.png">
<link rel="icon" type="image/png" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/robot_smaller_32x32.png">


   <title>NLPwShiyi - Natural Language Processing with Shiyi</title>  
</head>
<body>
  <!-- Latest compiled and minified CSS -->

<nav id="navbar" class="navigation" role="navigation">
  <input id="toggle1" type="checkbox" />
  <label class="hamburger1" for="toggle1">
    <div class="top"></div>
    <div class="meat"></div>
    <div class="bottom"></div>
  </label>

  <nav class="topnav mx-auto" id="myTopnav">
    <div class="dropdown">
      <button class="dropbtn">Comp Ling
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/1a-phil-of-mind">Philosophy of Mind</a> -->
        <a href="/modules/1b-info-theory">Information Theory</a>
        <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
        <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
        <a href="/modules/1e-mutual-info">Mutual Information</a>
        <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
        <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
        <a href="/modules/1h-semantics">Logic and Problem Solving</a>
        <!-- <a href="/modules/1i-cryptanalysis">Cryptography</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >DL / NLP
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/2a-mdn-nlp">Modern NLP</a> -->
        <a href="/modules/2b-markov-processes">Markov Processes</a>
        <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
        <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
        <a href="/modules/2f-loss-functions">Stochastic GD</a>
        <a href="/modules/2c-word2vec">Word2Vec</a>
        <a href="/modules/2g-batchnorm">Batchnorm</a>
        <a href="/modules/2j-perplexity">Perplexity</a>
        <a href="/modules/2h-dropout">Dropout</a>
        <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        <a href="/modules/2k-VAE">Variational Autoencoders</a>
        <a href="/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
      </div>
    </div>
    <a href="/" class="active">Intro </a>
    <div class="dropdown">
      <button class="dropbtn" >SOTA
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">

        <a href="/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a>
        <a href="/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a>
        <a href="/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a>
        <a href="/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a>
        <a href="/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a>
        
        <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >Hands-on
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
        <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
        <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
        <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a>
        <a href="/modules/4e-c4fe-tbip">Text-based Ideal Points Model</a>
        <!-- <a href="/modules/4f-server-build">WebServer Build Example</a> -->
        <a href="/modules/4g-etl-job">Serverless ETL Example</a>
        <a href="/modules/4h-ocr-data-aug">OCR Text Augmentation</a>
        <a href="/modules/4i-neo4j-gql">Neo4j GQL Example</a>
        <!-- <a href="/modules/4j-amr-parser">AMR Parser Example</a> -->


        

      </div>
    </div>
  </nav>
</nav>
  
  
<!-- Content appended here -->
  <!-- {{if hascode}} {{insert copy_paste.html}} {{end}} --><div class="franklin-content"><h1 id="loss_functions_for_classification"><a href="#loss_functions_for_classification" class="header-anchor">Loss Functions for Classification</a></h1>
<p><strong>Table of Contents</strong></p>
<div class="franklin-toc"><ol><li><a href="#minimal_working_examples">Minimal Working Examples</a><ol><li><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnbcelosshtmltorchnnbcelossbceloss"><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"><code>BCELoss</code></a></a></li><li><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnnlllosshtmltorchnnnlllossnllloss_and_a_hrefhttpspytorchorgdocsstablegeneratedtorchnncrossentropylosshtmltorchnncrossentropylosscrossentropyloss"><a href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"><code>NLLLoss</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"><code>CrossEntropyLoss</code></a></a></li></ol></li></ol></div>
<h1 id="general_ml_classification_tasks"><a href="#general_ml_classification_tasks" class="header-anchor">General ML Classification Tasks</a></h1>
<p>In this blog, general machine learning classification task will be covered. </p>
<p><strong>Some supervised learning basics</strong> </p>
<ul>
<li><p>Linear regression </p>
</li>
<li><p>Gradient descent algorithms</p>
</li>
<li><p>Logistic regression </p>
</li>
<li><p>Classification and softmax regression </p>
</li>
</ul>
<p>The training steps pretty much consist of the below flow:</p>
<ul>
<li><p>Dataset and Dataloader &#43; Model &#43; Loss and Optimizer &#61; Training </p>
</li>
</ul>
<p>Maximizing the log likelihood in the training step.</p>
<p><strong>A Probabilistic Model</strong></p>
<p>The dataset is made of \(m\) training examples \((x(i), y(i))_{i\in[m]}\), where </p>
\[\mathcal{L}(\theta \mid x) = \log L(\theta)
                             = -m\log (\sigma \sqrt{2\pi}) - \frac{1}{2\sigma^{2}}\sum_{i=1}^{m}(y(i) - \theta^T x(i))^2\]
<p>And the Jacobian vector product or cost function is :</p>
\[ J(\theta) = \frac{1}{2}\sum_{i=1}(y(i) - \theta^Tx(i))^2 \]
<p>giving rise to the ordinary lease squares regression model,</p>
<p>The gradient of the least squares cost function is:</p>
\[\frac{\partial}{\partial\theta} J(\theta) = \sum_{i=0}^{m} (y(i) - \theta^T x(i)) \frac{\partial}{\partial \theta_{j}} (y(i) - \sum_{i=0}^{d} \theta_{k}x_{k}(i)) = \sum_{i=0}^{m} (y(i) - \theta^{T} x(i)) = \sum_{i=0}^{m} (y(i) = \theta^{T} x(i))x_j(i)\]
<h1 id="gradient_descent_algorithms"><a href="#gradient_descent_algorithms" class="header-anchor">Gradient  Descent Algorithms</a></h1>
<p>Batch gradient descent performs the update </p>
\[ \theta_{j} := \theta_{j} + \alpha \sum_{i=0}^{m} (y(i) - \theta^Tx(i)x_{j}(i)) \]
<p>where \(\alpha\) is the learning rate,</p>
<p>This method looks at every example in the entire training set </p>
<p>Stochastic gradient descent works very well. The sum above is &quot;replaced&quot; by a loop over the training examples, so that the update becomes: </p>
<p>for \(i = 1\) to \(m\):</p>
\[\theta_{j} := \theta_{j} + \alpha (y(i) - \theta_{T}x(i)x_{j}(i))\]
<p>Linear regression: recall that under mild assumptions, the explicit solution for the ordinary least squares can be written explicitly as: </p>
\[\theta^{*} = (X^TX)^{-1}X^{T}Y\]
<p>where the linear model is written in matrix form \(Y = X\theta + \epsilon\), with \(Y = (y(1),...y(m)) \in \mathbb{R}^{m \times d}\)</p>
<p>In the context of linear regression, the log-likelihood is often associated with the assumption of normally distributed errors. The typical formulation assumes that the response variable follows a normal distribution with a mean determined by the linear regression model. Here&#39;s how you can express the log-likelihood for a simple linear regression model:</p>
<p>Assuming the response variable \((y_i)\) for each observation is normally distributed with mean \((\mu_i)\) and constant variance \((\sigma^2)\), the likelihood function for the observed data \((y_i)\) given the linear regression model is:</p>
\[ L(\beta_0, \beta_1, \sigma^2 \mid x_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right) \]
<p>where \((\mu_i = \beta_0 + \beta_1 x_i)\) is the mean predicted by the linear regression model for the \((i)\)-th observation.</p>
<p>The log-likelihood for the entire dataset \((\{y_1, y_2, \ldots, y_n\})\) is the sum of the log-likelihood contributions from each observation:</p>
\[ \mathcal{L}(\beta_0, \beta_1, \sigma^2 \mid x_i) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mu_i)^2 \]
<p>Here:</p>
<ul>
<li><p>\((\beta_0)\) and \((\beta_1)\) are the coefficients of the linear regression model.</p>
</li>
<li><p>\((n)\) is the number of observations.</p>
</li>
</ul>
<p>The goal in linear regression is often to maximize this log-likelihood function, which is equivalent to minimizing the sum of squared residuals &#40;ordinary least squares approach&#41;.</p>
<p>Note that in practice, maximizing the log-likelihood is often done under the assumption that the errors \((y_i - \mu_i)\) are normally distributed, which allows for the use of maximum likelihood estimation &#40;MLE&#41;. This assumption is a key aspect of classical linear regression.</p>
<p>Now let&#39;s take a look at logistic regression.</p>
<p>A natural &#40;generalized&#41; linear model for binary classification:</p>
\[ p_{\theta}(y=1 | x) = \sigma(\theta^{T}x)\]
\[ p_{\theta}(y=0 | x) = 1 - \sigma(\theta^{T}x)\]
<p>where \(\sigma(z) = \frac{1}{1+\epsilon^{-z}}\) is the sigmoid function &#40;or logistic function&#41;.</p>
<h1 id="insert_the_graph_here_todo"><a href="#insert_the_graph_here_todo" class="header-anchor">insert the graph here TODO</a></h1>
<p>The compact formula is \(p_{\theta}(y|x) = \sigma(\theta^{T}x)^y(1 - \theta(\theta^{T}x))^{(1-y)}\)</p>
<p>Logistic Regression: </p>
\[ L(\theta) = \prod_{i=1}^{m} \sigma(\theta^{T}x(i)^{y(i)}) \]
<p>There is no closed form formula for \(argmax \mathcal(\theta)\) so that we need now to use iterative algorithms. The gradient of the log likelihood is:</p>
\[\frac{\partial}{\partial_{j}} \mathcal{l}(\theta) = \sum_{i=1}^{m}(y(i) - \sigma(\theta^Tx(i))x_{j}(i)\]
<p>where we used the fact that \(\sigma'(z) = \sigma(z)(1 - \sigma(z))\)</p>
<p>Now we will break down the binary cross entropy loss function the <code>torch.nn.BCELoss</code> computes Binary Cross Entropy between the target \(y = (y(1),...,y(m)) \in {0, 1}^{m}\) and the output \(z = (z(1),...,z(m)) \in [0,1]^{m}\) as follows:</p>
\[ \text{loss}(i) = -[\text{y}(i)\text{log} \text{z}(i) + (1 - \text{y}(i)) \text{log}(1 - \text{z}(i))]\]
\[ \text{BCELoss}(z,y) = \frac{1}{m} \sum_{i=1}^{m} \text{loss}(i)\]
<p>In summary, we get </p>
\[\text{BCEWithLogitsLoss}(z,y) = \text{BCELoss}(\sigma(z),y)\]
<p>The version is more numerically stable.</p>
<p>Note the default \(1/m\), where \(m\) is typically the size of the batch. This factor will be directly multiplied with the learning rate. Recall the batch gradient descent update:</p>
\[\theta_{j} := \theta_{j} + \alpha \frac{\partial}{\partial \theta_{j}} \text{loss}(\theta) \]
<p>Softmax Regression: now we have \(c\) classes and for a training example &#40;x,y&#41;, the quantity \(\theta_{k}^{T}x\) should be related to the probability for the target \(y\) to belong to class \(k\). By analogy with the binary case, we assume:</p>
\[ \text{log} p_{\theta}(y = k | x) \approx \theta_{k}^{T}x, \text{for all} k = 1,...,c.\]
<p>as a consequence, we have with \(\theta = (\theta_{1},..,\theta_{c}) \in \mathbb{R}^{(dxc)}\):</p>
\[p_{\theta}(y = k | x) = \frac{e^{\theta_{k}^{T}x}}{\sum_{l} e^{\theta_{l}^{T}}x}\]
<p>and we can write it in vector form:</p>
\[(p_{\theta}(y = k | x)_{k=1,...,c} = softmax(\theta_{1}^{T}x),...,\theta_{c}^{T}x)\]
<p>where the sigmoid function is applied componentwise.</p>
<p>For the logistic regression, we had only one parameter \(\theta\) whereas here, for two classes we have two parameters: \(\theta_{1}\) and \(\theta_{2}\).</p>
<p>For 2 classes, we recover the logistic regression:</p>
\[ p_{\theta} ( y = 1 | x) = \frac{e_{1}^{T}x}{e_{1}^{T}x + e_{0}^{T}x} \]
\[                         = \frac{1}{1 + e^{\theta_{0}^{T} - \theta_{1}^{T}x}}\]
<p>Classification and softmax regression:</p>
<p>For the softmax regression, the log-likelihood can be written as:</p>
\[ \mathcal{l}(\theta) = \sum_{i=1}^{m} \sum_{k=1}^{c}(y(i)=k)\text{log}\left( \frac{e^{\theta_{k}^{T}}x(i)}{\sum_{l} e^{\theta_{l}^{T}}x(i)} \right)\]
\[ = \sum_{i=1}^{T} \text{log softmax}_{y(i)}(\theta_{1}^{T} ,..., \theta_{c}^{T}x(i)) \]
<p>In PyTorch, if the last layer of your network is a LogSoftmax&#40;&#41; function, then you can od a softmax regression with the <code>torch.nn.NLLoss&#40;&#41;</code>.</p>
<h2 id="minimal_working_examples"><a href="#minimal_working_examples" class="header-anchor">Minimal Working Examples</a></h2>
<h3 id="a_hrefhttpspytorchorgdocsstablegeneratedtorchnnbcelosshtmltorchnnbcelossbceloss"><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnbcelosshtmltorchnnbcelossbceloss" class="header-anchor"><a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"><code>BCELoss</code></a></a></h3>
<pre><code class="language-python">import torch.nn as nn
m &#61; nn.Sigmoid&#40;&#41;
loss &#61; nn.BCELoss&#40;&#41;
input &#61; torch.randn&#40;3,4,5&#41;
target &#61; torch.randn&#40;3,4,5&#41;
loss&#40;m&#40;input&#41;, target&#41;</code></pre>
<h3 id="a_hrefhttpspytorchorgdocsstablegeneratedtorchnnnlllosshtmltorchnnnlllossnllloss_and_a_hrefhttpspytorchorgdocsstablegeneratedtorchnncrossentropylosshtmltorchnncrossentropylosscrossentropyloss"><a href="#a_hrefhttpspytorchorgdocsstablegeneratedtorchnnnlllosshtmltorchnnnlllossnllloss_and_a_hrefhttpspytorchorgdocsstablegeneratedtorchnncrossentropylosshtmltorchnncrossentropylosscrossentropyloss" class="header-anchor"><a href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"><code>NLLLoss</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"><code>CrossEntropyLoss</code></a></a></h3>
<pre><code class="language-python">import torch.nn as nn
m &#61; nn.LogSoftmax&#40;dim&#61;1&#41;
loss1 &#61; nn.NLLLoss&#40;&#41;
loss2 &#61; nn.CrossEntropyLoss&#40;&#41;
C &#61; 8
input &#61; torch.randn&#40;3,C,4,5&#41;
target &#61; torch.empty&#40;3,4,5 dtype&#61;torch.long&#41;.random_&#40;0,C&#41; 
assert loss1&#40;m&#40;input&#41;,target&#41; &#61;&#61; loss2&#40;input,target&#41;</code></pre>
<div class="page-foot">
    <div class="copyright">
      <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo" src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
       ©️ Last modified: May 09, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    </div>
  </div>
  </div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
