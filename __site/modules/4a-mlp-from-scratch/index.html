<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   

  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/celeste.min.css">

<link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
<!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
<link rel="icon" type="image/png" sizes="152x152" href="/assets/robot_smaller_152x152.png">
<link rel="icon" type="image/x-icon" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/x-icon" sizes="32x32" href="/assets/robot_smaller_32x32.png">
<link rel="icon" type="image/png" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/robot_smaller_32x32.png">


   <title>NLPwShiyi - Natural Language Processing with Shiyi</title>  
</head>
<body>
  <!-- Latest compiled and minified CSS -->

<nav id="navbar" class="navigation" role="navigation">
  <input id="toggle1" type="checkbox" />
  <label class="hamburger1" for="toggle1">
    <div class="top"></div>
    <div class="meat"></div>
    <div class="bottom"></div>
  </label>

  <nav class="topnav mx-auto" id="myTopnav">
    <div class="dropdown">
      <button class="dropbtn">Comp Ling
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/1a-phil-of-mind">Philosophy of Mind</a> -->
        <a href="/modules/1b-info-theory">Information Theory</a>
        <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
        <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
        <a href="/modules/1e-mutual-info">Mutual Information</a>
        <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
        <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
        <a href="/modules/1h-semantics">Logic and Problem Solving</a>
        <!-- <a href="/modules/1i-cryptanalysis">Cryptography</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >DL / NLP
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/2a-mdn-nlp">Modern NLP</a> -->
        <a href="/modules/2b-markov-processes">Markov Processes</a>
        <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
        <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
        <a href="/modules/2f-loss-functions">Stochastic GD</a>
        <a href="/modules/2c-word2vec">Word2Vec</a>
        <a href="/modules/2g-batchnorm">Batchnorm</a>
        <a href="/modules/2j-perplexity">Perplexity</a>
        <a href="/modules/2h-dropout">Dropout</a>
        <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        <a href="/modules/2k-VAE">Variational Autoencoders</a>
        <a href="/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
      </div>
    </div>
    <a href="/" class="active">Intro </a>
    <div class="dropdown">
      <button class="dropbtn" >SOTA
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">

        <a href="/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a>
        <a href="/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a>
        <a href="/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a>
        <a href="/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a>
        <a href="/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a>
        
        <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >Hands-on
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
        <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
        <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
        <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a>
        <a href="/modules/4e-c4fe-tbip">Text-based Ideal Points Model</a>
        <!-- <a href="/modules/4f-server-build">WebServer Build Example</a> -->
        <a href="/modules/4g-etl-job">Serverless ETL Example</a>
        <a href="/modules/4h-ocr-data-aug">OCR Text Augmentation</a>
        <a href="/modules/4i-neo4j-gql">Neo4j GQL Example</a>
        <!-- <a href="/modules/4j-amr-parser">AMR Parser Example</a> -->


        

      </div>
    </div>
  </nav>
</nav>
  
  
<!-- Content appended here -->
  <!-- {{if hascode}} {{insert copy_paste.html}} {{end}} --><div class="franklin-content"><h1 id="the_downstream_of_a_mlp_from_scratch"><a href="#the_downstream_of_a_mlp_from_scratch" class="header-anchor">The Downstream of A MLP from Scratch </a></h1>
<p>In this blog, we will go step by step with how to create a mlp from scratch. </p>
<div class="franklin-toc"><ol><li><a href="#some_utility_functions_and_the_dataset">Some Utility Functions and The Dataset</a></li><li><a href="#mlp_in_numpy_and_define_the_grid_on_which_the_classifier_will_be_evaluated">MLP in NumPy and Define the Grid on Which The Classifier Will Be Evaluated</a></li><li><a href="#implementing_the_linear_layer">Implementing the Linear Layer </a></li><li><a href="#using_the_bce_loss">Using the BCE loss</a></li><li><a href="#using_a_pytorch_module">Using A Pytorch Module </a></li></ol></div>
<h2 id="some_utility_functions_and_the_dataset"><a href="#some_utility_functions_and_the_dataset" class="header-anchor">Some Utility Functions and The Dataset</a></h2>
<pre><code class="language-python"># all of these libraries are used for plotting
import numpy as np
import matplotlib.pyplot as plt

# Plot the dataset
def plot_data&#40;ax, X, Y&#41;:
    plt.axis&#40;&#39;off&#39;&#41;
    ax.scatter&#40;X&#91;:, 0&#93;, X&#91;:, 1&#93;, s&#61;1, c&#61;Y, cmap&#61;&#39;bone&#39;&#41;

from sklearn.datasets import make_moons
X, Y &#61; make_moons&#40;n_samples&#61;2000, noise&#61;0.1&#41;</code></pre>
<h2 id="mlp_in_numpy_and_define_the_grid_on_which_the_classifier_will_be_evaluated"><a href="#mlp_in_numpy_and_define_the_grid_on_which_the_classifier_will_be_evaluated" class="header-anchor">MLP in NumPy and Define the Grid on Which The Classifier Will Be Evaluated</a></h2>
<pre><code class="language-python">xx, yy &#61; np.meshgrid&#40;np.arange&#40;x_min, x_max, .1&#41;,
                     np.arange&#40;y_min, y_max, .1&#41;&#41;

to_forward &#61; np.array&#40;list&#40;zip&#40;xx.ravel&#40;&#41;, yy.ravel&#40;&#41;&#41;&#41;&#41;

# plot the decision boundary of our classifier


def plot_decision_boundary&#40;ax, X, Y, classifier&#41;:
    # forward pass on the grid, then convert to numpy for plotting
    Z &#61; classifier.forward&#40;to_forward&#41;
    Z &#61; Z.reshape&#40;xx.shape&#41;
    
    # plot contour lines of the values of our classifier on the grid
    ax.contourf&#40;xx, yy, Z&gt;0.5, cmap&#61;&#39;Blues&#39;&#41;
    
    # then plot the dataset
    plot_data&#40;ax, X,Y&#41;</code></pre>
<pre><code class="language-python"># Define the grid on which we will evaluate our classifier
xx, yy &#61; np.meshgrid&#40;np.arange&#40;x_min, x_max, .1&#41;,
                     np.arange&#40;y_min, y_max, .1&#41;&#41;

to_forward &#61; np.array&#40;list&#40;zip&#40;xx.ravel&#40;&#41;, yy.ravel&#40;&#41;&#41;&#41;&#41;

# plot the decision boundary of our classifier
def plot_decision_boundary&#40;ax, X, Y, classifier&#41;:
    # forward pass on the grid, then convert to numpy for plotting
    Z &#61; classifier.forward&#40;to_forward&#41;
    Z &#61; Z.reshape&#40;xx.shape&#41;
    
    # plot contour lines of the values of our classifier on the grid
    ax.contourf&#40;xx, yy, Z&gt;0.5, cmap&#61;&#39;Blues&#39;&#41;
    
    # then plot the dataset
    plot_data&#40;ax, X,Y&#41;</code></pre>
<h2 id="implementing_the_linear_layer"><a href="#implementing_the_linear_layer" class="header-anchor">Implementing the Linear Layer </a></h2>
<pre><code class="language-python">class MyReLU&#40;object&#41;:
    def forward&#40;self, x&#41;:
        # the relu is y_i &#61; max&#40;0, x_i&#41;
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
        
    
    def backward&#40;self, grad_output&#41;:
        # the gradient is 1 for the inputs that were above 0, 0 elsewhere
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
    
    def step&#40;self, learning_rate&#41;:
        # no need to do anything here, since ReLU has no parameters
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;

class MySigmoid&#40;object&#41;:
    def forward&#40;self, x&#41;:
        # the sigmoid is y_i &#61; 1./&#40;1&#43;exp&#40;-x_i&#41;&#41;
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
    
    def backward&#40;self, grad_output&#41;:
        # the partial derivative is e^-x / &#40;e^-x &#43; 1&#41;^2
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
    
    def step&#40;self, learning_rate&#41;:
        # no need to do anything here since Sigmoid has no parameters
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;</code></pre>
<pre><code class="language-python">class MyLinear&#40;object&#41;:
    def __init__&#40;self, n_input, n_output&#41;:
        # initialize two random matrices for W and b &#40;use np.random.randn&#41;
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;

    def forward&#40;self, x&#41;:
        # save a copy of x, you&#39;ll need it for the backward
        # return xW &#43; b
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;

    def backward&#40;self, grad_output&#41;:
        # y_i &#61; \sum_j x_j W_&#123;j,i&#125;  &#43; b_i
        # d y_i / d W_&#123;j, i&#125; &#61; x_j
        # d loss / d y_i &#61; grad_output&#91;i&#93;
        # so d loss / d W_&#123;j,i&#125; &#61; x_j * grad_output&#91;i&#93;  &#40;by the chain rule&#41;
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
        
        # d y_i / d b_i &#61; 1
        # d loss / d y_i &#61; grad_output&#91;i&#93;
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
        
        # now we need to compute the gradient with respect to x to
        # continue the back propagation d y_i / d x_j &#61; W_&#123;j, i&#125;
        # to compute the gradient of the loss, we have to sum over 
        # all possible y_i in the chain rule d loss / d x_j &#61; \sum_i 
        # &#40;d loss / d y_i&#41; &#40;d y_i / d x_j&#41;
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
    
    def step&#40;self, learning_rate&#41;:
        # update self.W and self.b in the opposite direction of the 
        # stored gradients, for learning_rate
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;</code></pre>
<h2 id="using_the_bce_loss"><a href="#using_the_bce_loss" class="header-anchor">Using the BCE loss</a></h2>
<pre><code class="language-python">class Sequential&#40;object&#41;:
    def __init__&#40;self, layers&#41;:
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
        
    def forward&#40;self, x&#41;:
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
    
    def compute_loss&#40;self, out, label&#41;:
        # use the BCE loss
        # -&#40;label * log&#40;output&#41; &#43; &#40;1-label&#41; * log&#40;1-output&#41;&#41;
        # save the gradient, and return the loss      
        # beware of dividing by zero in the gradient.
        # split the computation in two cases, one where the label is 
        # 0 and another one where the label is 1
        # add a small value &#40;1e-10&#41; to the denominator
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;

    def backward&#40;self&#41;:
        # apply backprop sequentially, starting from the gradient of the loss
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
    
    def step&#40;self, learning_rate&#41;:
        # take a gradient step for each layers
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;</code></pre>
<pre><code class="language-python">h&#61;50

# define your network with your Sequential
# it should be a linear layer with 2 inputs and h outputs, followed by a ReLU
# then a linear layer with h inputs and 1 outputs, followed by a sigmoid
# feel free to try other architectures

# YOUR CODE HERE
raise NotImplementedError&#40;&#41;</code></pre>
<pre><code class="language-python"># unfortunately animation is not working on colab
# you should comment the following line if on colab
&#37;matplotlib notebook
fig, ax &#61; plt.subplots&#40;1, 1, facecolor&#61;&#39;#4B6EA9&#39;&#41;
ax.set_xlim&#40;x_min, x_max&#41;
ax.set_ylim&#40;y_min, y_max&#41;
losses &#61; &#91;&#93;
learning_rate &#61; 1e-2
for it in range&#40;10000&#41;:
    # pick a random example id
    j &#61; np.random.randint&#40;1, len&#40;X&#41;&#41;

    # select the corresponding example and label
    example &#61; X&#91;j:j&#43;1&#93;
    label &#61; Y&#91;j&#93;

    # do a forward pass on the example
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # compute the loss according to your output and the label
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;
    
    # backward pass
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;
    
    # gradient step
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # draw the current decision boundary every 250 examples seen
    if it &#37; 250 &#61;&#61; 0 : 
        plot_decision_boundary&#40;ax, X,Y, net&#41;
        fig.canvas.draw&#40;&#41;
plot_decision_boundary&#40;ax, X,Y, net&#41;
fig.canvas.draw&#40;&#41;</code></pre>
<pre><code class="language-python">&#37;matplotlib inline
plt.plot&#40;losses&#41;</code></pre>
<h2 id="using_a_pytorch_module"><a href="#using_a_pytorch_module" class="header-anchor">Using A Pytorch Module </a></h2>
<pre><code class="language-python">import torch
import torch.nn as nn

# y &#61; xw &#43; b
class MyLinear_mod&#40;nn.Module&#41;:
    def __init__&#40;self, n_input, n_output&#41;:
        super&#40;MyLinear_mod, self&#41;.__init__&#40;&#41;
        # define self.A and self.b the weights and biases
        # initialize them with a normal distribution
        # use nn.Parameters
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;

    def forward&#40;self, x&#41;:
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;
        
class MyReLU_mod&#40;nn.Module&#41;:
    def __init__&#40;self&#41;:
        super&#40;MyReLU_mod, self&#41;.__init__&#40;&#41;
        
    def forward&#40;self, x&#41;:
        # YOUR CODE HERE
        raise NotImplementedError&#40;&#41;</code></pre>
<p><strong>Subsequent section defines the network using MyLinear<em>mod, MyReLU</em>mod and nn.Sigmoid</strong></p>
<pre><code class="language-python">from torch import optim
optimizer &#61; optim.SGD&#40;net.parameters&#40;&#41;, lr&#61;1e-2&#41;

X_torch &#61; torch.from_numpy&#40;X&#41;.float&#40;&#41;
Y_torch &#61; torch.from_numpy&#40;Y&#41;.float&#40;&#41;

# you should comment the following line if on colab
&#37;matplotlib notebook
fig, ax &#61; plt.subplots&#40;1, 1, facecolor&#61;&#39;#4B6EA9&#39;&#41;
ax.set_xlim&#40;x_min, x_max&#41;
ax.set_ylim&#40;y_min, y_max&#41;

losses &#61; &#91;&#93;
criterion &#61; nn.BCELoss&#40;&#41;
for it in range&#40;10000&#41;:
    # pick a random example id 
    j &#61; np.random.randint&#40;1, len&#40;X&#41;&#41;

    # select the corresponding example and label
    example &#61; X_torch&#91;j:j&#43;1&#93;
    label &#61; Y_torch&#91;j:j&#43;1&#93;.unsqueeze&#40;1&#41;

    # do a forward pass on the example
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # compute the loss according to your output and the label
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # zero the gradients
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # backward pass
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # gradient step
    # YOUR CODE HERE
    raise NotImplementedError&#40;&#41;

    # draw the current decision boundary every 250 examples seen
    if it &#37; 250 &#61;&#61; 0 : 
        plot_decision_boundary&#40;ax, X,Y, net&#41;
        fig.canvas.draw&#40;&#41;
plot_decision_boundary&#40;ax, X,Y, net&#41;
fig.canvas.draw&#40;&#41;
&#37;matplotlib inline
plt.plot&#40;losses&#41;</code></pre>
<div class="page-foot">
    <div class="copyright">
      <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo" src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
       ©️ Last modified: May 09, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    </div>
  </div>
  </div><!-- CONTENT ENDS HERE -->
    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
