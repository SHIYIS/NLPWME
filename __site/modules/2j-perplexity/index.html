<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
  

  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css">
<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/celeste.min.css">

<link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
<!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
<link rel="icon" type="image/png" sizes="152x152" href="/assets/robot_smaller_152x152.png">
<link rel="icon" type="image/x-icon" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/x-icon" sizes="32x32" href="/assets/robot_smaller_32x32.png">
<link rel="icon" type="image/png" sizes="64x64" href="/assets/robot_smaller_64x64.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/robot_smaller_32x32.png">


   <title>NLPwShiyi - Natural Language Processing with Shiyi</title>  
</head>
<body>
  <!-- Latest compiled and minified CSS -->

<nav id="navbar" class="navigation" role="navigation">
  <input id="toggle1" type="checkbox" />
  <label class="hamburger1" for="toggle1">
    <div class="top"></div>
    <div class="meat"></div>
    <div class="bottom"></div>
  </label>

  <nav class="topnav mx-auto" id="myTopnav">
    <div class="dropdown">
      <button class="dropbtn">Comp Ling
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/1a-phil-of-mind">Philosophy of Mind</a> -->
        <a href="/modules/1b-info-theory">Information Theory</a>
        <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
        <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
        <a href="/modules/1e-mutual-info">Mutual Information</a>
        <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
        <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
        <a href="/modules/1h-semantics">Logic and Problem Solving</a>
        <!-- <a href="/modules/1i-cryptanalysis">Cryptography</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >DL / NLP
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <!-- <a href="/modules/2a-mdn-nlp">Modern NLP</a> -->
        <a href="/modules/2b-markov-processes">Markov Processes</a>
        <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
        <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
        <a href="/modules/2f-loss-functions">Stochastic GD</a>
        <a href="/modules/2c-word2vec">Word2Vec</a>
        <a href="/modules/2g-batchnorm">Batchnorm</a>
        <a href="/modules/2j-perplexity">Perplexity</a>
        <a href="/modules/2h-dropout">Dropout</a>
        <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        <a href="/modules/2k-VAE">Variational Autoencoders</a>
        <a href="/modules/2l-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
      </div>
    </div>
    <a href="/" class="active">Intro </a>
    <div class="dropdown">
      <button class="dropbtn" >SOTA
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">

        <a href="/modules/3a-transformers"> GPT (Generative Pre-trained Transformer) </a>
        <a href="/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a>
        <a href="/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a>
        <a href="/modules/3d-t5"> T5 (Text-to-Text Transfer Transformer) </a>
        <a href="/modules/3e-clip"> CLIP (Contrastive Language-Image Pre-training) </a>
        
        <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
      </div>
    </div>
    <div class="dropdown">
      <button class="dropbtn" >Hands-on
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-content">
        <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
        <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
        <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
        <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF Seq2Seq</a>
        <a href="/modules/4e-c4fe-tbip">Text-based Ideal Points Model</a>
        <!-- <a href="/modules/4f-server-build">WebServer Build Example</a> -->
        <a href="/modules/4g-etl-job">Serverless ETL Example</a>
        <a href="/modules/4h-ocr-data-aug">OCR Text Augmentation</a>
        <a href="/modules/4i-neo4j-gql">Neo4j GQL Example</a>
        <!-- <a href="/modules/4j-amr-parser">AMR Parser Example</a> -->


        

      </div>
    </div>
  </nav>
</nav>
  
  
<!-- Content appended here -->
  <!-- {{if hascode}} {{insert copy_paste.html}} {{end}} --><div class="franklin-content"><h2 id="perplexity_and_how_its_applied_in_natural_language_processing"><a href="#perplexity_and_how_its_applied_in_natural_language_processing" class="header-anchor">Perplexity and How It&#39;s Applied in Natural Language Processing </a></h2>
<p>Perplexity is a measure used in natural language processing &#40;NLP&#41; to evaluate the performance of a language model. It quantifies how well the model predicts a sample of text.</p>
<p>Here&#39;s a breakdown of perplexity:</p>
<h3 id="definition"><a href="#definition" class="header-anchor">Definition:</a></h3>
<p>Perplexity measures how well a probability model predicts a sample. It reflects how surprised the model is when it sees new data. A lower perplexity indicates that the model is better at predicting the sample.</p>
<h3 id="formula"><a href="#formula" class="header-anchor">Formula:</a></h3>
<p>Perplexity is calculated as the inverse probability of the test set, normalized by the number of words:</p>
\[ \text{Perplexity}(w_1^N) = \sqrt[N]{\frac{1}{P(w_1^N)}} \]
<p>Where:</p>
<ul>
<li><p>\( N \) is the number of words in the test set.</p>
</li>
<li><p>\( P(w_1^N) \) is the probability of the test set under the model.</p>
</li>
</ul>
<p>Alternatively, you may see it represented using the log probability:</p>
\[ \text{Perplexity}(w_1^N) = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1^{i-1})\right) \]
<p>This formula calculates the geometric mean of the inverse probability of the words in the test set.</p>
<h3 id="application"><a href="#application" class="header-anchor">Application:</a></h3>
<ol>
<li><p><strong>Language Modeling Evaluation</strong>: Perplexity is commonly used to evaluate the performance of language models. Lower perplexity indicates better performance. Language models are trained to minimize perplexity on a given training dataset, and then this metric is used to evaluate their performance on unseen data.</p>
</li>
<li><p><strong>Model Comparison</strong>: Perplexity can be used to compare different language models. Given the same dataset, the model with lower perplexity is considered better.</p>
</li>
<li><p><strong>Hyperparameter Tuning</strong>: Perplexity can guide hyperparameter tuning during the training of language models. For example, it can help determine the optimal number of hidden units or layers in a neural network-based language model.</p>
</li>
<li><p><strong>Cross-Validation</strong>: Perplexity is also used in cross-validation setups to assess how well a model generalizes to unseen data.</p>
</li>
<li><p><strong>Human Evaluation</strong>: Although not as common, perplexity can also be used as a proxy for human evaluation of language generation tasks. However, it&#39;s important to note that low perplexity does not necessarily mean human-like or high-quality language generation.</p>
</li>
</ol>
<p>In summary, perplexity is a useful metric for assessing the performance of language models, particularly in the context of predicting sequences of words. It&#39;s widely used in NLP research and applications for model training, evaluation, and comparison.</p>
<div class="page-foot">
    <div class="copyright">
      <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo" src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
       ©️ Last modified: May 09, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    </div>
  </div>
  </div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
