<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">

  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700"
    type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/celeste.min.css">

  <link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
  <!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/apple-touch-icon.png">


  <title>NLPwShiyi - Natural Language Processing with Shiyi</title>
</head>

<body>
  <!-- Latest compiled and minified CSS -->

  <nav id="navbar" class="navigation" role="navigation">
    <input id="toggle1" type="checkbox" />
    <label class="hamburger1" for="toggle1">
      <div class="top"></div>
      <div class="meat"></div>
      <div class="bottom"></div>
    </label>

    <nav class="topnav mx-auto" id="myTopnav">
      <div class="dropdown">
        <button class="dropbtn">Comp Ling
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/1a-info-theory">Information Theory</a>
          <a href="/modules/1b-phil-of-mind">Philosophy of Mind</a>
          <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
          <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
          <a href="/modules/1e-mutual-info">Mutual Information</a>
          <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
          <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
          <a href="/modules/1h-semantics">Logic and Problem Solving</a>
          <a href="/modules/1i-cryptanalysis">Cryptography</a>
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">DL / NLP
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/2a-mdn-nlp">Modern NLP</a>
          <a href="/modules/2b-markov-processes">Markov Processes</a>
          <a href="/modules/2c-word2vec">Word2Vec</a>
          <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
          <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
          <a href="/modules/2f-loss-functions">Stochastic GD</a>
          <a href="/modules/2g-batchnorm">Batchnorm</a>
          <a href="/modules/2h-dropout">Dropout</a>
          <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        </div>
      </div>
      <a href="/" class="active">Intro </a>
      <div class="dropdown">
        <button class="dropbtn">SOTA
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/3a-VAE">Variational Autoencoders</a>
          <a href="/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
          <a href="/modules/3c-transformers">Transformers</a>
          <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">Hands-on
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
          <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
          <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
          <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a>
          <a href="/modules/4e-c4fe-tbip">C4FE-TBIP</a>
          <a href="/modules/4f-etl-job">DE: Serverless ETL</a>

        </div>
      </div>
    </nav>
  </nav>

  <!-- Content appended here -->
  <div class="franklin-content">
    <h1 id="automatic_differentiation_what_is_it_and_why_do_we_need_it"><a
        href="#automatic_differentiation_what_is_it_and_why_do_we_need_it" class="header-anchor">Automatic
        Differentiation: What Is It and Why Do We Need It? </a></h1>
    <p><strong>Table of Contents</strong></p>
    <div class="franklin-toc">
      <ol>
        <li><a href="#technicalities_of_automatic_differentiation">Technicalities of Automatic Differentiation</a></li>
      </ol>
    </div>
    <p>Automatic differentiation &#40;AD&#41; is a crucial technique in the field of machine learning for optimizing
      models through the training process. Here are some reasons why automatic differentiation is essential in this
      context:</p>
    <blockquote>
      <p><strong>Efficient Gradient Computation</strong>: Autodiff allows you to compute gradients efficiently without
        manually deriving and implementing them.</p>
    </blockquote>
    <p>Sample python code: </p>
    <pre><code class="language-python">import tensorflow as tf

x &#61; tf.constant&#40;2.0&#41;

with tf.GradientTape&#40;&#41; as tape:
    y &#61; x**2

dy_dx &#61; tape.gradient&#40;y, x&#41;
print&#40;dy_dx.numpy&#40;&#41;&#41;  # Output: 4.0</code></pre>
    <hr />
    <blockquote>
      <p><strong>Higher-order Gradients</strong>: Autodiff can easily compute higher-order derivatives without much
        additional effort.</p>
    </blockquote>
    <p>Sample python code: </p>
    <pre><code class="language-julia">import tensorflow as tf

x &#61; tf.constant&#40;2.0&#41;

with tf.GradientTape&#40;&#41; as tape1:
    with tf.GradientTape&#40;&#41; as tape2:
        y &#61; x**3
    dy_dx &#61; tape2.gradient&#40;y, x&#41;

d2y_dx2 &#61; tape1.gradient&#40;dy_dx, x&#41;
print&#40;d2y_dx2.numpy&#40;&#41;&#41;  # Output: 12.0</code></pre>
    <hr />
    <blockquote>
      <p><strong>Optimization with Gradient Descent</strong>: Autodiff facilitates gradient-based optimization
        algorithms like gradient descent.</p>
    </blockquote>
    <p>Sample python code:</p>
    <pre><code class="language-julia">import tensorflow as tf

x &#61; tf.Variable&#40;3.0, trainable&#61;True&#41;
y &#61; x**2

optimizer &#61; tf.optimizers.SGD&#40;learning_rate&#61;0.1&#41;

for _ in range&#40;100&#41;:
    with tf.GradientTape&#40;&#41; as tape:
        y &#61; x**2
    gradients &#61; tape.gradient&#40;y, x&#41;
    optimizer.apply_gradients&#40;&#91;&#40;gradients, x&#41;&#93;&#41;

print&#40;x.numpy&#40;&#41;&#41;  # Output: close to 0.0 &#40;minimum of y&#61;x^2&#41;</code></pre>
    <hr />
    <blockquote>
      <p><strong>Neural Network Training</strong>: Autodiff is essential for training neural networks efficiently by
        computing gradients for the backpropagation algorithm.</p>
    </blockquote>
    <p>Sample python code:</p>
    <pre><code class="language-julia">import tensorflow as tf

# Define a simple neural network
model &#61; tf.keras.Sequential&#40;&#91;
        tf.keras.layers.Dense&#40;10, activation&#61;&#39;relu&#39;, input_shape&#61;&#40;5,&#41;&#41;,
        tf.keras.layers.Dense&#40;1&#41;&#93;&#41;

# Define a sample dataset
data &#61; tf.constant&#40;tf.random.normal&#40;&#40;100, 5&#41;&#41;&#41;
labels &#61; tf.constant&#40;tf.random.normal&#40;&#40;100, 1&#41;&#41;&#41;

# Training loop
optimizer &#61; tf.optimizers.Adam&#40;learning_rate&#61;0.01&#41;

for epoch in range&#40;100&#41;:
    with tf.GradientTape&#40;&#41; as tape:
        predictions &#61; model&#40;data&#41;
        loss &#61; tf.losses.mean_squared_error&#40;labels, predictions&#41;

    gradients &#61; tape.gradient&#40;loss, model.trainable_variables&#41;
    optimizer.apply_gradients&#40;zip&#40;gradients, model.trainable_variables&#41;&#41;

# Model is now trained</code></pre>
    <hr />
    <p>Below attached is a video of one of the lessons from the CMU deep learning classes cmu10714 I find really helpful
      and useful. It&#39;s one of the steps of a simple implementation of a ML Module called NEEDLE, and in this video
      the important gists of automatic differentiation is covered. </p>
    <h2 id="technicalities_of_automatic_differentiation"><a href="#technicalities_of_automatic_differentiation"
        class="header-anchor">Technicalities of Automatic Differentiation</a></h2>
    <p>
      <div id="videoContainer">
        <div id="player"></div>
      </div>
      <script>
        var tag = document.createElement('script');

        tag.src = "https://www.youtube.com/iframe_api";
        var firstScriptTag = document.getElementsByTagName('script')[0];
        firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

        var player;

        function onYouTubeIframeAPIReady() {
          player = new YT.Player('player', {
            height: '300',
            width: '100%',
            videoId: 'cNADlHfHQHg',
            playerVars: {
              'autoplay': 0,
              'rel': 0,
              'cc_load_policy': 1
            }
          });
        }

        function changeYouTubeSource(startTime, endTime) {

          var youtubeIframe = document.getElementById('player');

          var youtubeIframeSrc = document.getElementById('player').getAttribute('src');

          var trimmedIframeUrl = '';
          var iframeUrlTimeStamp = '';

          if (youtubeIframeSrc.match(/&start=/g)) {
            var mediaFragmentIndex = youtubeIframeSrc.indexOf('&start=');
            trimmedIframeUrl = youtubeIframeSrc.slice(0, mediaFragmentIndex);

            if (endTime === 0) {
              iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime;
            } else {
              iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime + '&end=' + endTime;
            }
          }

          if (youtubeIframeSrc.match(/&start=/g) === null) {
            if (endTime === 0) {
              iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime;
            } else {
              iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime + '&end=' + endTime;
            }
          }

          setTimeout(function () {

            var iframeAutoplayUrl = iframeUrlTimeStamp.replace('autoplay=0', 'autoplay=1');

            youtubeIframe.setAttribute('src', iframeAutoplayUrl);
          }, 1000);
        }

      </script>
      <br>
      <a href='#player' onclick='changeYouTubeSource(0,0)'> 0:00</a> Introduction
      <br>
      <a href='#player' onclick='changeYouTubeSource(222,0)'> 3:42</a> The NEEDLE Module
      <br>
      <a href='#player' onclick='changeYouTubeSource(649,0)'> 10:49</a> Codebase and Data Structures
      <br>
      <a href='#player' onclick='changeYouTubeSource(860,0)'> 14:20</a> Computational Graph
      <br>
      <a href='#player' onclick='changeYouTubeSource(1779,0)'> 29:39</a> Executing The Computation
      <br>
      <a href='#player' onclick='changeYouTubeSource(2758,0)'> 45:58</a> Scenarios Where Things Might Go Amok
      <br>
      <a href='#player' onclick='changeYouTubeSource(3137,0)'> 52:17</a> Reverse Mode Auto Diff
    </p>
    <div class="page-foot">
      <div class="copyright">
        <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo"
              src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
        ©️ Last modified: March 06, 2024. Website built with <a
          href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia
          programming language</a>.
      </div>
    </div>
  </div><!-- CONTENT ENDS HERE -->


  <script src="/libs/highlight/highlight.min.js"></script>
  <script>
    hljs.highlightAll();
    hljs.configure({
      tabReplace: '    '
    });

  </script>


</body>

</html>
