<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=apple-touch-icon-precomposed  sizes=152x152  href="/nlpwme/assets/apple-touch-icon.png"> <title>NLPwShiyi - Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1a-info-theory">Information Theory</a> <a href="/nlpwme/modules/1b-phil-of-mind">Philosophy of Mind</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1i-cryptanalysis">Cryptography</a> </div> </div> <div class=dropdown > <button class=dropbtn  >DL / NLP <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2a-mdn-nlp">Modern NLP</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3a-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> <a href="/nlpwme/modules/3c-transformers">Transformers</a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a> <a href="/nlpwme/modules/4e-c4fe-tbip">C4FE-TBIP</a> <a href="/nlpwme/modules/4f-etl-job">DE: Serverless ETL</a> </div> </div> </nav> </nav> <div class=franklin-content ><h1 id=automatic_differentiation_what_is_it_and_why_do_we_need_it ><a href="#automatic_differentiation_what_is_it_and_why_do_we_need_it" class=header-anchor >Automatic Differentiation: What Is It and Why Do We Need It? </a></h1> <p><strong>Table of Contents</strong></p> <div class=franklin-toc ><ol><li><a href="#technicalities_of_automatic_differentiation">Technicalities of Automatic Differentiation</a></ol></div> <p>Automatic differentiation &#40;AD&#41; is a crucial technique in the field of machine learning for optimizing models through the training process. Here are some reasons why automatic differentiation is essential in this context:</p> <blockquote> <p><strong>Efficient Gradient Computation</strong>: Autodiff allows you to compute gradients efficiently without manually deriving and implementing them.</p> </blockquote> <p>Sample python code: </p> <pre><code class="python hljs"><span class=hljs-keyword >import</span> tensorflow <span class=hljs-keyword >as</span> tf

x = tf.constant(<span class=hljs-number >2.0</span>)

<span class=hljs-keyword >with</span> tf.GradientTape() <span class=hljs-keyword >as</span> tape:
    y = x**<span class=hljs-number >2</span>

dy_dx = tape.gradient(y, x)
<span class=hljs-built_in >print</span>(dy_dx.numpy())  <span class=hljs-comment ># Output: 4.0</span></code></pre> <hr /> <blockquote> <p><strong>Higher-order Gradients</strong>: Autodiff can easily compute higher-order derivatives without much additional effort.</p> </blockquote> <p>Sample python code: </p> <pre><code class="julia hljs"><span class=hljs-keyword >import</span> tensorflow as tf

x = tf.constant(<span class=hljs-number >2.0</span>)

with tf.GradientTape() as tape1:
    with tf.GradientTape() as tape2:
        y = x**<span class=hljs-number >3</span>
    dy_dx = tape2.gradient(y, x)

d2y_dx2 = tape1.gradient(dy_dx, x)
print(d2y_dx2.numpy())  <span class=hljs-comment ># Output: 12.0</span></code></pre> <hr /> <blockquote> <p><strong>Optimization with Gradient Descent</strong>: Autodiff facilitates gradient-based optimization algorithms like gradient descent.</p> </blockquote> <p>Sample python code:</p> <pre><code class="julia hljs"><span class=hljs-keyword >import</span> tensorflow as tf

x = tf.Variable(<span class=hljs-number >3.0</span>, trainable=True)
y = x**<span class=hljs-number >2</span>

optimizer = tf.optimizers.SGD(learning_rate=<span class=hljs-number >0.1</span>)

<span class=hljs-keyword >for</span> _ <span class=hljs-keyword >in</span> range(<span class=hljs-number >100</span>):
    with tf.GradientTape() as tape:
        y = x**<span class=hljs-number >2</span>
    gradients = tape.gradient(y, x)
    optimizer.apply_gradients([(gradients, x)])

print(x.numpy())  <span class=hljs-comment ># Output: close to 0.0 (minimum of y=x^2)</span></code></pre> <hr /> <blockquote> <p><strong>Neural Network Training</strong>: Autodiff is essential for training neural networks efficiently by computing gradients for the backpropagation algorithm.</p> </blockquote> <p>Sample python code:</p> <pre><code class="julia hljs"><span class=hljs-keyword >import</span> tensorflow as tf

<span class=hljs-comment ># Define a simple neural network</span>
model = tf.keras.Sequential([
        tf.keras.layers.Dense(<span class=hljs-number >10</span>, activation=&#x27;relu&#x27;, input_shape=(<span class=hljs-number >5</span>,)),
        tf.keras.layers.Dense(<span class=hljs-number >1</span>)])

<span class=hljs-comment ># Define a sample dataset</span>
data = tf.constant(tf.random.normal((<span class=hljs-number >100</span>, <span class=hljs-number >5</span>)))
labels = tf.constant(tf.random.normal((<span class=hljs-number >100</span>, <span class=hljs-number >1</span>)))

<span class=hljs-comment ># Training loop</span>
optimizer = tf.optimizers.Adam(learning_rate=<span class=hljs-number >0.01</span>)

<span class=hljs-keyword >for</span> epoch <span class=hljs-keyword >in</span> range(<span class=hljs-number >100</span>):
    with tf.GradientTape() as tape:
        predictions = model(data)
        loss = tf.losses.mean_squared_error(labels, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

<span class=hljs-comment ># Model is now trained</span></code></pre> <hr /> <p>Below attached is a video of one of the lessons from the CMU deep learning classes cmu10714 I find really helpful and useful. It&#39;s one of the steps of a simple implementation of a ML Module called NEEDLE, and in this video the important gists of automatic differentiation is covered. </p> <h2 id=technicalities_of_automatic_differentiation ><a href="#technicalities_of_automatic_differentiation" class=header-anchor >Technicalities of Automatic Differentiation</a></h2> <p> <div id=videoContainer  > <div id=player ></div> </div> <script> var tag = document.createElement('script'); tag.src = "https://www.youtube.com/iframe_api"; var firstScriptTag = document.getElementsByTagName('script')[0]; firstScriptTag.parentNode.insertBefore(tag, firstScriptTag); var player; function onYouTubeIframeAPIReady() { player = new YT.Player('player', { height: '300', width: '100%', videoId: 'cNADlHfHQHg', playerVars: { 'autoplay': 0, 'rel': 0, 'cc_load_policy': 1 } }); } function changeYouTubeSource(startTime, endTime) { var youtubeIframe = document.getElementById('player'); var youtubeIframeSrc = document.getElementById('player').getAttribute('src'); var trimmedIframeUrl = ''; var iframeUrlTimeStamp = ''; if (youtubeIframeSrc.match(/&start=/g)) { var mediaFragmentIndex = youtubeIframeSrc.indexOf('&start='); trimmedIframeUrl = youtubeIframeSrc.slice(0, mediaFragmentIndex); if (endTime === 0) { iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime; } else { iframeUrlTimeStamp = trimmedIframeUrl + '&start=' + startTime + '&end=' + endTime; } } if (youtubeIframeSrc.match(/&start=/g) === null) { if (endTime === 0) { iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime; } else { iframeUrlTimeStamp = youtubeIframeSrc + '&start=' + startTime + '&end=' + endTime; } } setTimeout(function() { var iframeAutoplayUrl = iframeUrlTimeStamp.replace('autoplay=0', 'autoplay=1'); youtubeIframe.setAttribute('src', iframeAutoplayUrl); }, 1000); } </script> <br> <a href='#player' onclick='changeYouTubeSource(0,0)'> 0:00</a> Introduction <br> <a href='#player' onclick='changeYouTubeSource(222,0)'> 3:42</a> The NEEDLE Module <br> <a href='#player' onclick='changeYouTubeSource(649,0)'> 10:49</a> Codebase and Data Structures <br> <a href='#player' onclick='changeYouTubeSource(860,0)'> 14:20</a> Computational Graph <br> <a href='#player' onclick='changeYouTubeSource(1779,0)'> 29:39</a> Executing The Computation <br> <a href='#player' onclick='changeYouTubeSource(2758,0)'> 45:58</a> Scenarios Where Things Might Go Amok <br> <a href='#player' onclick='changeYouTubeSource(3137,0)'> 52:17</a> Reverse Mode Auto Diff </p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: March 20, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>