@def sequence = ["optim-basics"]

# Module 4 - Optimization for deep leaning


**Table of Contents**

\toc


## Optimization for deep leaning

{{youtube_placeholder optim-basics}}

{{yt_tsp 0 0 Recap}}
{{yt_tsp 31 0 Plan}}
{{yt_tsp 74 0 Optimization in deep learning}}
{{yt_tsp 224 0 Gradient descent variants}}
{{yt_tsp 478 0 Setting for the jupyter notebook}}
{{yt_tsp 589 0 Vanilla gradient descent}}
{{yt_tsp 734 0 Momentum}}
{{yt_tsp 938 0 Nesterov accelerated gradient descent}}
{{yt_tsp 1080 0 Adagrad}}
{{yt_tsp 1206 0 RMSProp}}
{{yt_tsp 1331 0 Adam}}
{{yt_tsp 1479 0 AMSGrad}}
{{yt_tsp 1629 0 Pytorch optimizers}}

## Slides and Practicals

- [slides](https://dataflowr.github.io/slides/module4.html)
- [notebook](https://github.com/dataflowr/notebooks/blob/master/Module4/04_gradient_descent_optimization_algorithms_empty.ipynb) in [colab](https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module4/04_gradient_descent_optimization_algorithms_empty.ipynb) Code your optimizers.

## References

- [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747) by Sebastian Ruder
- [Gradient-based optimization](https://drive.google.com/file/d/1e_9W8q9PL20iqOR-pfK89eILc_VtYaw1/view) A short introduction to optimization in Deep Learning, by Christian S. Perone
