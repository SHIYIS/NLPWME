<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/nlpwme/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/nlpwme/css/franklin.css"> <link rel=stylesheet  href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700" type="text/css"> <link rel=stylesheet  href="/nlpwme/css/font-awesome.min.css"> <link rel=stylesheet  href="/nlpwme/css/celeste.min.css"> <link rel=icon  type="image/png" sizes=200x200  href="/nlpwme/assets/robot.png"> <link rel=apple-touch-icon-precomposed  sizes=152x152  href="/nlpwme/assets/apple-touch-icon.png"> <title>NLPwShiyi - Natural Language Processing with Shiyi</title> <nav id=navbar  class=navigation  role=navigation > <input id=toggle1  type=checkbox  /> <label class=hamburger1  for=toggle1 > <div class=top ></div> <div class=meat ></div> <div class=bottom ></div> </label> <nav class="topnav mx-auto" id=myTopnav > <div class=dropdown > <button class=dropbtn >Comp Ling <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/1a-info-theory">Information Theory</a> <a href="/nlpwme/modules/1b-phil-of-mind">Philosophy of Mind</a> <a href="/nlpwme/modules/1c-noisy-channel-model">The Noisy Channel Model</a> <a href="/nlpwme/modules/1d-finite-automata">FSAs and FSTs</a> <a href="/nlpwme/modules/1e-mutual-info">Mutual Information</a> <a href="/nlpwme/modules/1f-cky-algorithm">CKY Algorithm</a> <a href="/nlpwme/modules/1g-viterbi">Viterbi Algorithm</a> <a href="/nlpwme/modules/1h-semantics">Logic and Problem Solving</a> <a href="/nlpwme/modules/1i-cryptanalysis">Cryptography</a> </div> </div> <div class=dropdown > <button class=dropbtn  >DL / NLP <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/2a-mdn-nlp">Modern NLP</a> <a href="/nlpwme/modules/2b-markov-processes">Markov Processes</a> <a href="/nlpwme/modules/2c-word2vec">Word2Vec</a> <a href="/nlpwme/modules/2e-jax">Jacobian Matrices Derivation</a> <a href="/nlpwme/modules/2d-automatic-differentiation">Automatic Differentiation</a> <a href="/nlpwme/modules/2f-loss-functions">Stochastic GD</a> <a href="/nlpwme/modules/2g-batchnorm">Batchnorm</a> <a href="/nlpwme/modules/2j-perplexity">Perplexity</a> <a href="/nlpwme/modules/2h-dropout">Dropout</a> <a href="/nlpwme/modules/2i-depth">Depth: Pros and Cons</a> <a href="/nlpwme/modules/3a-VAE">Variational Autoencoders</a> <a href="/nlpwme/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a> </div> </div> <a href="/nlpwme/" class=active >Intro </a> <div class=dropdown > <button class=dropbtn  >SOTA <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/3c-transformers"> GPT (Generative Pre-trained Transformer) </a> <a href="/nlpwme/modules/3b-xlnet"> XLNet (Generalized Autoregressive Pretraining) </a> <a href="/nlpwme/modules/3c-roberta"> RoBERTa (Robustly optimized BERT approach) </a> <a href="/nlpwme/modules/3c-transformers"> T5 (Text-to-Text Transfer Transformer) </a> <a href="/nlpwme/modules/3c-transformers"> CLIP (Contrastive Language-Image Pre-training) </a> </div> </div> <div class=dropdown > <button class=dropbtn  >Hands-on <i class="fa fa-caret-down"></i> </button> <div class=dropdown-content > <a href="/nlpwme/modules/4a-mlp-from-scratch">MLP from Scratch</a> <a href="/nlpwme/modules/4b-generative-adversarial-networks">GAN Example </a> <a href="/nlpwme/modules/4c-vae-mnist">VAE for MNIST</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a> <a href="/nlpwme/modules/4d-bi-lstm-crf">OCR Handwriting Recognition</a> <a href="/nlpwme/modules/4e-c4fe-tbip">Text-based Ideal Points Model</a> <a href="/nlpwme/modules/4f-etl-job">DE: Serverless ETL</a> <a href="/nlpwme/modules/4f-etl-job">PySpark SQL Example</a> </div> </div> </nav> </nav> <div class=franklin-content ><h2 id=xlnet_generalized_autoregressive_pretraining_for_language_understanding ><a href="#xlnet_generalized_autoregressive_pretraining_for_language_understanding" class=header-anchor >XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></h2> <p>XLNet &#40;eXtreme Learning Machine&#41; is a state-of-the-art language model developed by Google AI. It builds upon the Transformer architecture, which is a deep learning model specifically designed for sequence-to-sequence tasks such as language modeling.</p> <p><a href="https://arxiv.org/abs/1906.08237">Here</a> is the original link to the paper. </p> <p>Here&#39;s a breakdown of XLNet along with necessary formulas, explanations, and potential code examples:</p> <ol> <li><p><strong>Transformer Architecture</strong>: XLNet utilizes the Transformer architecture as its backbone. The key components of the Transformer architecture include:</p> <p>a. <strong>Self-Attention Mechanism</strong>: This mechanism allows the model to weigh the importance of each word/token in the context of the entire sequence. It computes attention scores between all pairs of words in a sequence and uses these scores to create context-aware representations for each word.</p> <p>b. <strong>Positional Encoding</strong>: Since Transformers do not inherently understand the order of words in a sequence, positional encodings are added to the input embeddings to provide positional information to the model.</p> <p>c. <strong>Feedforward Neural Networks</strong>: After obtaining contextualized representations through self-attention, Transformer layers typically pass the representations through feedforward neural networks to capture more complex patterns.</p> <li><p><strong>Permutation Language Modeling &#40;PLM&#41;</strong>: XLNet introduces Permutation Language Modeling, which differs from traditional autoregressive language modeling used in models like GPT &#40;Generative Pre-trained Transformer&#41;. In PLM, instead of conditioning on previous words sequentially, the model conditions on all permutations of the input tokens. This allows XLNet to learn bidirectional relationships between tokens.</p> <li><p><strong>Training Objective</strong>: XLNet uses a modified version of the autoregressive training objective used in models like GPT. The objective is to maximize the expected log-likelihood of the target sequence given the input sequence. Mathematically, it can be represented as:</p> <p>&#91; \max<em>&#123;\theta&#125; \mathbb&#123;E&#125;</em>&#123;&#40;x,y&#41; \sim \mathcal&#123;D}} \sum<em>&#123;t&#61;1&#125;^&#123;T</em>y&#125; \log P&#40;y<em>t | x, y</em>&#123;&lt;t&#125;; \theta&#41; &#93;</p> <p>Where:</p> <ul> <li><p>&#40; &#40;x, y&#41; &#41; represents a training example consisting of an input sequence &#40; x &#41; and its corresponding target sequence &#40; y &#41;.</p> <li><p>&#40; T_y &#41; is the length of the target sequence.</p> <li><p>&#40; \theta &#41; represents the model parameters.</p> <li><p>&#40; P&#40;y<em>t | x, y</em>&#123;&lt;t&#125;; \theta&#41; &#41; is the conditional probability of the target token &#40; y<em>t &#41; given the input sequence &#40; x &#41; and previously generated tokens &#40; y</em>&#123;&lt;t&#125; &#41;.</p> </ul> <li><p><strong>Implementation</strong>: XLNet can be implemented using various deep learning frameworks such as TensorFlow or PyTorch. Below is a simplified PyTorch code snippet demonstrating how to use XLNet for text generation:</p> </ol> <pre><code class="python hljs"><span class=hljs-keyword >import</span> torch
<span class=hljs-keyword >from</span> transformers <span class=hljs-keyword >import</span> XLNetLMHeadModel, XLNetTokenizer

<span class=hljs-comment ># Load pre-trained XLNet model and tokenizer</span>
model_name = <span class=hljs-string >&#x27;xlnet-base-cased&#x27;</span>
tokenizer = XLNetTokenizer.from_pretrained(model_name)
model = XLNetLMHeadModel.from_pretrained(model_name)

<span class=hljs-comment ># Input text</span>
input_text = <span class=hljs-string >&quot;The cat sat on the&quot;</span>

<span class=hljs-comment ># Tokenize input text</span>
input_ids = tokenizer.encode(input_text, add_special_tokens=<span class=hljs-literal >False</span>, return_tensors=<span class=hljs-string >&quot;pt&quot;</span>)

<span class=hljs-comment ># Generate text using XLNet</span>
max_length = <span class=hljs-number >50</span>
output = model.generate(input_ids, max_length=max_length, num_return_sequences=<span class=hljs-number >1</span>)

<span class=hljs-comment ># Decode generated tokens</span>
generated_text = tokenizer.decode(output[<span class=hljs-number >0</span>], skip_special_tokens=<span class=hljs-literal >True</span>)
<span class=hljs-built_in >print</span>(<span class=hljs-string >&quot;Generated text:&quot;</span>, generated_text)</code></pre> <p>This code demonstrates how to generate text using a pre-trained XLNet model. You first need to load the model and tokenizer, tokenize your input text, generate text using the model&#39;s <code>generate</code> method, and finally decode the generated tokens to obtain the output text.</p> <p>This breakdown provides a high-level overview of XLNet, its key components, training objective, and a simple code example for text generation. For more advanced usage and fine-tuning for specific tasks, additional considerations and modifications may be required.</p> <h2 id=bert_vs_xlnet ><a href="#bert_vs_xlnet" class=header-anchor >BERT vs XLNet</a></h2> <p>BERT &#40;Bidirectional Encoder Representations from Transformers&#41; and XLNet are both state-of-the-art language models, but they differ in their approach to handling bidirectionality and context modeling. Here&#39;s an explanation of how BERT lacks bidirectional context modeling and how XLNet addresses this issue:</p> <ol> <li><p><strong>BERT&#39;s Masked Language Model &#40;MLM&#41;</strong>:</p> <ul> <li><p>BERT is pre-trained using a Masked Language Model objective, where a percentage of the input tokens are randomly masked, and the model is trained to predict these masked tokens based on the surrounding context.</p> <li><p>While BERT captures bidirectional context within a single training instance &#40;i.e., it can see both left and right context during pre-training&#41;, it lacks the ability to capture bidirectional dependencies across multiple training instances. This is because each training instance &#40;sentence or segment&#41; is processed independently.</p> </ul> <li><p><strong>XLNet&#39;s Permutation Language Modeling &#40;PLM&#41;</strong>:</p> <ul> <li><p>XLNet, on the other hand, introduces Permutation Language Modeling, which is an improvement over BERT&#39;s approach. Instead of masking tokens as in BERT, XLNet conditions on all possible permutations of the input tokens.</p> <li><p>By considering all permutations, XLNet can learn bidirectional relationships between tokens across multiple training instances. This enables XLNet to capture richer contextual information and dependencies compared to BERT.</p> <li><p>Furthermore, XLNet maintains BERT&#39;s ability to capture bidirectional context within a single training instance by considering all permutations of the input tokens, including the original order.</p> </ul> <li><p><strong>Improvements by XLNet</strong>:</p> <ul> <li><p>XLNet&#39;s PLM addresses the limitations of BERT by explicitly modeling bidirectional dependencies across multiple training instances.</p> <li><p>XLNet achieves this by considering all possible permutations of the input tokens during training, allowing it to capture a more comprehensive understanding of the text&#39;s context.</p> <li><p>As a result, XLNet tends to outperform BERT on various downstream NLP tasks that require a deeper understanding of context and dependencies across sentences or documents.</p> </ul> </ol> <p>In summary, while BERT captures bidirectional context within a single training instance through masked language modeling, it lacks the ability to model bidirectional dependencies across multiple instances. XLNet addresses this limitation by introducing Permutation Language Modeling, which enables it to capture bidirectional relationships across multiple instances, leading to improved performance on various NLP tasks.</p> <div class=page-foot > <div class=copyright > <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a> ©️ Last modified: March 27, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> <script src="/nlpwme/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>