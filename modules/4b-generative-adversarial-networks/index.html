<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

  <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">

  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700"
    type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/celeste.min.css">

  <link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
  <!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/apple-touch-icon.png">


  <title>NLPwShiyi - Natural Language Processing with Shiyi</title>
</head>

<body>
  <!-- Latest compiled and minified CSS -->

  <nav id="navbar" class="navigation" role="navigation">
    <input id="toggle1" type="checkbox" />
    <label class="hamburger1" for="toggle1">
      <div class="top"></div>
      <div class="meat"></div>
      <div class="bottom"></div>
    </label>

    <nav class="topnav mx-auto" id="myTopnav">
      <div class="dropdown">
        <button class="dropbtn">Comp Ling
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/1a-info-theory">Information Theory</a>
          <a href="/modules/1b-phil-of-mind">Philosophy of Mind</a>
          <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
          <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
          <a href="/modules/1e-mutual-info">Mutual Information</a>
          <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
          <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
          <a href="/modules/1h-semantics">Logic and Problem Solving</a>
          <a href="/modules/1i-cryptanalysis">Cryptography</a>
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">DL / NLP
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/2a-mdn-nlp">Modern NLP</a>
          <a href="/modules/2b-markov-processes">Markov Processes</a>
          <a href="/modules/2c-word2vec">Word2Vec</a>
          <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
          <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
          <a href="/modules/2f-loss-functions">Stochastic GD</a>
          <a href="/modules/2g-batchnorm">Batchnorm</a>
          <a href="/modules/2h-dropout">Dropout</a>
          <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        </div>
      </div>
      <a href="/" class="active">Intro </a>
      <div class="dropdown">
        <button class="dropbtn">SOTA
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/3a-VAE">Variational Autoencoders</a>
          <a href="/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
          <a href="/modules/3c-transformers">Transformers</a>
          <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">Hands-on
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
          <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
          <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
          <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a>
          <a href="/modules/4e-c4fe-tbip">C4FE-TBIP</a>
          <a href="/modules/4f-etl-job">DE: Serverless ETL</a>

        </div>
      </div>
    </nav>
  </nav>

  <!-- Content appended here -->
  <div class="franklin-content">
    <h1 id="generative_adversarial_networks"><a href="#generative_adversarial_networks" class="header-anchor">Generative
        Adversarial Networks</a></h1>
    <p><strong>Table of Contents</strong></p>
    <div class="franklin-toc">
      <ol>
        <li><a href="#generative_adversarial_networks__2">Generative Adversarial Networks</a></li>
        <li><a href="#a_simple_gan">A Simple GAN</a></li>
        <li><a href="#conditional_gan">Conditional GAN</a></li>
        <li><a href="#info_gan">Info GAN</a></li>
        <li><a href="#variational_autoencoders">Variational Autoencoders</a></li>
      </ol>
    </div>
    <h2 id="generative_adversarial_networks__2"><a href="#generative_adversarial_networks__2"
        class="header-anchor">Generative Adversarial Networks</a></h2>
    <p>In this section, we play with the GAN described in the lesson on a double moon dataset.</p>
    <p>Then we implement a Conditional GAN and an InfoGAN.</p>
    <pre><code class="language-python"># all of these libraries are used for plotting
import numpy as np
import matplotlib.pyplot as plt

# Plot the dataset
def plot_data&#40;ax, X, Y, color &#61; &#39;bone&#39;&#41;:
    plt.axis&#40;&#39;off&#39;&#41;
    ax.scatter&#40;X&#91;:, 0&#93;, X&#91;:, 1&#93;, s&#61;1, c&#61;Y, cmap&#61;color&#41;


from sklearn.datasets import make_moons
X, y &#61; make_moons&#40;n_samples&#61;2000, noise&#61;0.05&#41;


n_samples &#61; X.shape&#91;0&#93;
Y &#61; np.ones&#40;n_samples&#41;
fig, ax &#61; plt.subplots&#40;1, 1,facecolor&#61;&#39;#4B6EA9&#39;&#41;

plot_data&#40;ax, X, Y&#41;
plt.show&#40;&#41;


import torch
device &#61; torch.device&#40;cuda:0 if torch.cuda.is_available&#40;&#41; else cpu&#41;

print&#40;&#39;Using gpu: &#37;s &#39; &#37; torch.cuda.is_available&#40;&#41;&#41;</code></pre>
    <h2 id="a_simple_gan"><a href="#a_simple_gan" class="header-anchor">A Simple GAN</a></h2>
    <p>We start with the simple GAN described in the course.</p>
    <pre><code class="language-python">import torch.nn as nn

z_dim &#61; 32
hidden_dim &#61; 128

net_G &#61; nn.Sequential&#40;nn.Linear&#40;z_dim,hidden_dim&#41;,
                     nn.ReLU&#40;&#41;, nn.Linear&#40;hidden_dim, 2&#41;&#41;

net_D &#61; nn.Sequential&#40;nn.Linear&#40;2,hidden_dim&#41;,
                     nn.ReLU&#40;&#41;,
                     nn.Linear&#40;hidden_dim,1&#41;,
                     nn.Sigmoid&#40;&#41;&#41;

net_G &#61; net_G.to&#40;device&#41;
net_D &#61; net_D.to&#40;device&#41;</code></pre>
    <p>Training loop as described here, keeping the losses for the discriminator and the generator.</p>
    <pre><code class="language-python">batch_size &#61; 50
lr &#61; 1e-4
nb_epochs &#61; 500

optimizer_G &#61; torch.optim.Adam&#40;net_G.parameters&#40;&#41;,lr&#61;lr&#41;
optimizer_D &#61; torch.optim.Adam&#40;net_D.parameters&#40;&#41;,lr&#61;lr&#41;

loss_D_epoch &#61; &#91;&#93;
loss_G_epoch &#61; &#91;&#93;
for e in range&#40;nb_epochs&#41;:
    np.random.shuffle&#40;X&#41;
    real_samples &#61; torch.from_numpy&#40;X&#41;.type&#40;torch.FloatTensor&#41;
    loss_G &#61; 0
    loss_D &#61; 0
    for t, real_batch in enumerate&#40;real_samples.split&#40;batch_size&#41;&#41;:
            #improving D
        z &#61; torch.empty&#40;batch_size,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
        fake_batch &#61; net_G&#40;z&#41;
        D_scores_on_real &#61; net_D&#40;real_batch.to&#40;device&#41;&#41;
        D_scores_on_fake &#61; net_D&#40;fake_batch&#41;
            
        loss &#61; -torch.mean&#40;torch.log&#40;1-D_scores_on_fake&#41; &#43; torch.log&#40;D_scores_on_real&#41;&#41;
        optimizer_D.zero_grad&#40;&#41;
        loss.backward&#40;&#41;
        optimizer_D.step&#40;&#41;
        loss_D &#43;&#61; loss.cpu&#40;&#41;.data.numpy&#40;&#41;
                    
            # improving G
        z &#61; torch.empty&#40;batch_size,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
        fake_batch &#61; net_G&#40;z&#41;
        D_scores_on_fake &#61; net_D&#40;fake_batch&#41;
            
        loss &#61; -torch.mean&#40;torch.log&#40;D_scores_on_fake&#41;&#41;
        optimizer_G.zero_grad&#40;&#41;
        loss.backward&#40;&#41;
        optimizer_G.step&#40;&#41;
        loss_G &#43;&#61; loss.cpu&#40;&#41;.data.numpy&#40;&#41;
           
    loss_D_epoch.append&#40;loss_D&#41;
    loss_G_epoch.append&#40;loss_G&#41;


plt.plot&#40;loss_D_epoch&#41;
plt.plot&#40;loss_G_epoch&#41;


z &#61; torch.empty&#40;n_samples,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
fake_samples &#61; net_G&#40;z&#41;
fake_data &#61; fake_samples.cpu&#40;&#41;.data.numpy&#40;&#41;

fig, ax &#61; plt.subplots&#40;1, 1, facecolor&#61;&#39;#4B6EA9&#39;&#41;
all_data &#61; np.concatenate&#40;&#40;X,fake_data&#41;,axis&#61;0&#41;
Y2 &#61; np.concatenate&#40;&#40;np.ones&#40;n_samples&#41;,np.zeros&#40;n_samples&#41;&#41;&#41;
plot_data&#40;ax, all_data, Y2&#41;
plt.show&#40;&#41;;

# It looks like the GAN is oscillating. Try again with lr&#61;1e-3

# We can generate more points

z &#61; torch.empty&#40;10*n_samples,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
fake_samples &#61; net_G&#40;z&#41;
fake_data &#61; fake_samples.cpu&#40;&#41;.data.numpy&#40;&#41;
fig, ax &#61; plt.subplots&#40;1, 1, facecolor&#61;&#39;#4B6EA9&#39;&#41;
all_data &#61; np.concatenate&#40;&#40;X,fake_data&#41;,axis&#61;0&#41;
Y2 &#61; np.concatenate&#40;&#40;np.ones&#40;n_samples&#41;,np.zeros&#40;10*n_samples&#41;&#41;&#41;
plot_data&#40;ax, all_data, Y2&#41;
plt.show&#40;&#41;;</code></pre>
    <h2 id="conditional_gan"><a href="#conditional_gan" class="header-anchor">Conditional GAN</a></h2>
    <p>We are now implementing a conditional GAN. We start by separating the two half moons in two clusters as follows:
    </p>
    <pre><code class="language-python">X, Y &#61; make_moons&#40;n_samples&#61;2000, noise&#61;0.05&#41;
n_samples &#61; X.shape&#91;0&#93;
fig, ax &#61; plt.subplots&#40;1, 1, facecolor&#61;&#39;#4B6EA9&#39;&#41;
plot_data&#40;ax, X, Y&#41;
plt.show&#40;&#41;</code></pre>
    <p>The task is now given a white or black label to generate points in the corresponding cluster.</p>
    <p>Both the generator and the discriminator take in addition a one hot encoding of the label. The generator will now
      generate fake points corresponding to the input label. The discriminator, given a pair of sample and label should
      detect if this is a fake or a real pair.</p>
    <pre><code class="language-python">z_dim &#61; 32
hidden_dim &#61; 128
label_dim &#61; 2


class generator&#40;nn.Module&#41;:
    def __init__&#40;self,z_dim &#61; z_dim, label_dim&#61;label_dim,hidden_dim &#61;hidden_dim&#41;:
        super&#40;generator,self&#41;.__init__&#40;&#41;
        self.net &#61; nn.Sequential&#40;nn.Linear&#40;z_dim&#43;label_dim,hidden_dim&#41;,
                     nn.ReLU&#40;&#41;, nn.Linear&#40;hidden_dim, 2&#41;&#41;
        
    def forward&#40;self, input, label_onehot&#41;:
        x &#61; torch.cat&#40;&#91;input, label_onehot&#93;, 1&#41;
        return self.net&#40;x&#41;
    
class discriminator&#40;nn.Module&#41;:
    def __init__&#40;self,z_dim &#61; z_dim, label_dim&#61;label_dim,hidden_dim &#61;hidden_dim&#41;:
        super&#40;discriminator,self&#41;.__init__&#40;&#41;
        self.net &#61;  nn.Sequential&#40;nn.Linear&#40;2&#43;label_dim,hidden_dim&#41;,
                     nn.ReLU&#40;&#41;,
                     nn.Linear&#40;hidden_dim,1&#41;,
                     nn.Sigmoid&#40;&#41;&#41;
        
    def forward&#40;self, input, label_onehot&#41;:
        x &#61; torch.cat&#40;&#91;input, label_onehot&#93;, 1&#41;
        return self.net&#40;x&#41;
        

net_CG &#61; generator&#40;&#41;.to&#40;device&#41;
net_CD &#61; discriminator&#40;&#41;.to&#40;device&#41;</code></pre>
    <p>You need to code the training loop:</p>
    <pre><code class="language-python">batch_size &#61; 50
lr &#61; 1e-3
nb_epochs &#61; 1000

optimizer_CG &#61; torch.optim.Adam&#40;net_CG.parameters&#40;&#41;,lr&#61;lr&#41;
optimizer_CD &#61; torch.optim.Adam&#40;net_CD.parameters&#40;&#41;,lr&#61;lr&#41;
loss_D_epoch &#61; &#91;&#93;
loss_G_epoch &#61; &#91;&#93;
for e in range&#40;nb_epochs&#41;:
    rperm &#61; np.random.permutation&#40;X.shape&#91;0&#93;&#41;;
    np.take&#40;X,rperm,axis&#61;0,out&#61;X&#41;;
    np.take&#40;Y,rperm,axis&#61;0,out&#61;Y&#41;;
    real_samples &#61; torch.from_numpy&#40;X&#41;.type&#40;torch.FloatTensor&#41;
    real_labels &#61; torch.from_numpy&#40;Y&#41;.type&#40;torch.LongTensor&#41;
    loss_G &#61; 0
    loss_D &#61; 0
    for real_batch, real_batch_label in zip&#40;real_samples.split&#40;batch_size&#41;,real_labels.split&#40;batch_size&#41;&#41;:
            #improving D
        z &#61; torch.empty&#40;batch_size,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
        #
        # your code here
        # hint: https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/4
        #
                
        loss &#61; -torch.mean&#40;torch.log&#40;1-D_scores_on_fake&#41; &#43; torch.log&#40;D_scores_on_real&#41;&#41;
        optimizer_CD.zero_grad&#40;&#41;
        loss.backward&#40;&#41;
        optimizer_CD.step&#40;&#41;
        loss_D &#43;&#61; loss.cpu&#40;&#41;.data.numpy&#40;&#41;
            
            # improving G
        z &#61; torch.empty&#40;batch_size,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
        #
        # your code here
        #
                    
        loss &#61; -torch.mean&#40;torch.log&#40;D_scores_on_fake&#41;&#41;
        optimizer_CG.zero_grad&#40;&#41;
        loss.backward&#40;&#41;
        optimizer_CG.step&#40;&#41;
        loss_G &#43;&#61; loss.cpu&#40;&#41;.data.numpy&#40;&#41;
                    
    loss_D_epoch.append&#40;loss_D&#41;
    loss_G_epoch.append&#40;loss_G&#41;</code></pre>
    <pre><code class="language-python">plt.plot&#40;loss_D_epoch&#41;
plt.plot&#40;loss_G_epoch&#41;</code></pre>
    <pre><code class="language-python">z &#61; torch.empty&#40;n_samples,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
label &#61; torch.LongTensor&#40;n_samples,1&#41;.random_&#40;&#41; &#37; label_dim
label_onehot &#61; torch.FloatTensor&#40;n_samples, label_dim&#41;.zero_&#40;&#41;
label_onehot &#61; label_onehot.scatter_&#40;1, label, 1&#41;.to&#40;device&#41;
fake_samples &#61; net_CG&#40;z, label_onehot&#41;
fake_data &#61; fake_samples.cpu&#40;&#41;.data.numpy&#40;&#41;</code></pre>
    <h2 id="info_gan"><a href="#info_gan" class="header-anchor">Info GAN</a></h2>
    <p>Here we implement a simplified version of the algorithm presented in the InfoGAN paper.</p>
    <p>This time, you do not have access to the labels but you know there are two classes. The idea is then to provide
      as in the conditional GAN a random label to the generator but in opposition to the conditional GAN, the
      discriminator cannot take as input the label &#40;since they are not provided to us&#41; but instead the
      discriminator will predict a label and this prediction can be trained on fake samples only&#33;</p>
    <pre><code class="language-python">import torch.nn.functional as F

z_dim &#61; 32
hidden_dim &#61; 128
label_dim &#61; 2


class Igenerator&#40;nn.Module&#41;:
    def __init__&#40;self,z_dim &#61; z_dim, label_dim&#61;label_dim,hidden_dim &#61;hidden_dim&#41;:
        super&#40;Igenerator,self&#41;.__init__&#40;&#41;
        self.net &#61; nn.Sequential&#40;nn.Linear&#40;z_dim&#43;label_dim,hidden_dim&#41;,
                     nn.ReLU&#40;&#41;, nn.Linear&#40;hidden_dim, 2&#41;&#41;
        
    def forward&#40;self, input, label_onehot&#41;:
        x &#61; torch.cat&#40;&#91;input, label_onehot&#93;, 1&#41;
        return self.net&#40;x&#41;
    
class Idiscriminator&#40;nn.Module&#41;:
    def __init__&#40;self,z_dim &#61; z_dim, label_dim&#61;label_dim,hidden_dim &#61;hidden_dim&#41;:
        super&#40;Idiscriminator,self&#41;.__init__&#40;&#41;
        self.fc1 &#61; nn.Linear&#40;2,hidden_dim&#41;
        self.fc2 &#61; nn.Linear&#40;hidden_dim,1&#41;
        self.fc3 &#61; nn.Linear&#40;hidden_dim,1&#41;
        
    def forward&#40;self, input&#41;:
        x &#61; F.relu&#40;self.fc1&#40;input&#41;&#41;
        output &#61; torch.sigmoid&#40;self.fc2&#40;x&#41;&#41;
        est_label &#61; torch.sigmoid&#40;self.fc3&#40;x&#41;&#41; 
        return output, est_label
        

net_IG &#61; Igenerator&#40;&#41;.to&#40;device&#41;
net_ID &#61; Idiscriminator&#40;&#41;.to&#40;device&#41;</code></pre>
    <p><strong>Here, we add loss_fn which is the BCELoss to be used for the binary classification task of the
        discriminator on the fake samples.</strong></p>
    <pre><code class="language-python">batch_size &#61; 50
lr &#61; 1e-3
nb_epochs &#61; 1000
loss_fn &#61; nn.BCELoss&#40;&#41;

optimizer_IG &#61; torch.optim.Adam&#40;net_IG.parameters&#40;&#41;,lr&#61;lr&#41;
optimizer_ID &#61; torch.optim.Adam&#40;net_ID.parameters&#40;&#41;,lr&#61;lr&#41;
loss_D_epoch &#61; &#91;&#93;
loss_G_epoch &#61; &#91;&#93;
for e in range&#40;nb_epochs&#41;:
    
    rperm &#61; np.random.permutation&#40;X.shape&#91;0&#93;&#41;;
    np.take&#40;X,rperm,axis&#61;0,out&#61;X&#41;;
    #np.take&#40;Y,rperm,axis&#61;0,out&#61;Y&#41;;
    real_samples &#61; torch.from_numpy&#40;X&#41;.type&#40;torch.FloatTensor&#41;
    #real_labels &#61; torch.from_numpy&#40;Y&#41;.type&#40;torch.LongTensor&#41;
    loss_G &#61; 0
    loss_D &#61; 0
    for real_batch in real_samples.split&#40;batch_size&#41;:
            #improving D
        z &#61; torch.empty&#40;batch_size,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
        #
        # your code here
        #
        
            
            # improving G
        z &#61; torch.empty&#40;batch_size,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
        #
        # your code here
        #
               
            
    loss_D_epoch.append&#40;loss_D&#41;
    loss_G_epoch.append&#40;loss_G&#41;


plt.plot&#40;loss_D_epoch&#41;
plt.plot&#40;loss_G_epoch&#41;


z &#61; torch.empty&#40;n_samples,z_dim&#41;.normal_&#40;&#41;.to&#40;device&#41;
label &#61; torch.LongTensor&#40;n_samples,1&#41;.random_&#40;&#41; &#37; label_dim
label_onehot &#61; torch.FloatTensor&#40;n_samples, label_dim&#41;.zero_&#40;&#41;
label_onehot &#61; label_onehot.scatter_&#40;1, label, 1&#41;.to&#40;device&#41;
fake_samples &#61; net_IG&#40;z, label_onehot&#41;
fake_data &#61; fake_samples.cpu&#40;&#41;.data.numpy&#40;&#41;</code></pre>
    <h2 id="variational_autoencoders"><a href="#variational_autoencoders" class="header-anchor">Variational
        Autoencoders</a></h2>
    <p>Consider a latent variable model with a data variable \(x\in \mathcal{X}\) and a latent variable \(z\in
      \mathcal{Z}\), \(p(z,x) = p(z)p_\theta(x|z)\). Given the data \(x_1,\dots, x_n\), we want to train the model by
      maximizing the marginal log-likelihood:</p>
    <div class="nonumber">\[\begin{array}{rcl}
      \mathcal{L} = \mathbf{E}_{p_d(x)}\left[\log p_\theta(x)\right]=\mathbf{E}_{p_d(x)}\left[\log
      \int_{\mathcal{Z}}p_{\theta}(x|z)p(z)dz\right]
      \end{array}\]</div>
    <p>where \(p_d\) denotes the empirical distribution of \(X\): \(p_d(x) =\frac{1}{n}\sum_{i=1}^n \delta_{x_i}(x)\).
    </p>
    <p>To avoid the &#40;often&#41; difficult computation of the integral above, the idea behind variational methods is
      to instead maximize a lower bound to the log-likelihood:</p>
    <div class="nonumber">\[\begin{array}{rcl}

      \mathcal{L} \geq L(p_\theta(x|z),q(z|x)) =\mathbf{E}_{p_d(x)}\left[\mathbf{E}_{q(z|x)}\left[\log
      p_\theta(x|z)\right]-\mathrm{KL}\left( q(z|x)||p(z)\right)\right]
      \end{array}\]</div>
    <p>Any choice of \(q(z|x)\) gives a valid lower bound. Variational autoencoders replace the variational posterior
      \(q(z|x)\) by an inference network \(q_{\phi}(z|x)\) that is trained together with \(p_{\theta}(x|z)\) to jointly
      maximize \(L(p_\theta,q_\phi)\).</p>
    <p>The variational posterior \(q_{\phi}(z|x)\) is also called the <strong>encoder</strong> and the generative model
      \(p_{\theta}(x|z)\), the <strong>decoder</strong> or generator.</p>
    <p>The first term \(\mathbf{E}_{q(z|x)}\left[\log p_\theta(x|z)\right]\) is the negative reconstruction error.
      Indeed under a gaussian assumption i.e. \(p_{\theta}(x|z) = \mathcal{N}(\mu_{\theta}(z), I)\) the term \(\log
      p_\theta(x|z)\) reduces to \(\propto \|x-\mu_\theta(z)\|^2\), which is often used in practice. The term
      \(\mathrm{KL}\left( q(z|x)||p(z)\right)\) can be seen as a regularization term, where the variational posterior
      \(q_\phi(z|x)\) should be matched to the prior \(p(z)= \mathcal{N}(0, I)\).</p>
    <p>Variational Autoencoders were introduced by <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling
        &#40;2013&#41;</a>, see also <a href="https://arxiv.org/abs/1606.05908">&#40;Doersch, 2016&#41;</a> for a
      tutorial.</p>
    <p>There are various examples of VAE in PyTorch available <a
        href="https://github.com/pytorch/examples/tree/master/vae">here</a> or <a
        href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65">here</a>.
      The code below is taken from this last source.</p>
    <div class="page-foot">
      <div class="copyright">
        <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo"
              src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
        ©️ Last modified: March 12, 2024. Website built with <a
          href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia
          programming language</a>.
      </div>
    </div>
  </div><!-- CONTENT ENDS HERE -->

  <script src="/libs/katex/katex.min.js"></script>
  <script src="/libs/katex/contrib/auto-render.min.js"></script>
  <script>
    renderMathInElement(document.body)

  </script>



  <script src="/libs/highlight/highlight.min.js"></script>
  <script>
    hljs.highlightAll();
    hljs.configure({
      tabReplace: '    '
    });

  </script>


</body>

</html>
