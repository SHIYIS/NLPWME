<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">


  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700"
    type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/celeste.min.css">

  <link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
  <!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/apple-touch-icon.png">


  <title>NLPwShiyi - Natural Language Processing with Shiyi</title>
</head>

<body>
  <!-- Latest compiled and minified CSS -->

  <nav id="navbar" class="navigation" role="navigation">
    <input id="toggle1" type="checkbox" />
    <label class="hamburger1" for="toggle1">
      <div class="top"></div>
      <div class="meat"></div>
      <div class="bottom"></div>
    </label>

    <nav class="topnav mx-auto" id="myTopnav">
      <div class="dropdown">
        <button class="dropbtn">Comp Ling
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/1a-info-theory">Information Theory</a>
          <a href="/modules/1b-phil-of-mind">Philosophy of Mind</a>
          <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
          <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
          <a href="/modules/1e-mutual-info">Mutual Information</a>
          <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
          <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
          <a href="/modules/1h-semantics">Logic and Problem Solving</a>
          <a href="/modules/1i-cryptanalysis">Cryptography</a>
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">DL / NLP
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/2a-mdn-nlp">Modern NLP</a>
          <a href="/modules/2b-markov-processes">Markov Processes</a>
          <a href="/modules/2c-word2vec">Word2Vec</a>
          <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
          <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
          <a href="/modules/2f-loss-functions">Stochastic GD</a>
          <a href="/modules/2g-batchnorm">Batchnorm</a>
          <a href="/modules/2h-dropout">Dropout</a>
          <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        </div>
      </div>
      <a href="/" class="active">Intro </a>
      <div class="dropdown">
        <button class="dropbtn">SOTA
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/3a-VAE">Variational Autoencoders</a>
          <a href="/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
          <a href="/modules/3c-transformers">Transformers</a>
          <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">Hands-on
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
          <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
          <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
          <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a>
          <a href="/modules/4e-c4fe-tbip">C4FE-TBIP</a>
          <a href="/modules/4f-etl-job">DE: Serverless ETL</a>

        </div>
      </div>
    </nav>
  </nav>

  <!-- Content appended here -->
  <div class="franklin-content">
    <h1 id="what_is_mutual_information"><a href="#what_is_mutual_information" class="header-anchor">What is Mutual
        Information? </a></h1>
    <p>A non-negative metric known as mutual information &#40;MI&#41; is used to assess how closely two random variables
      depend on one another. The reciprocal information measures how much we can learn about a second variable by
      looking at the values of the first.</p>
    <p>Because it can assess non-linear relationships as well as linear ones, the mutual information is a useful
      substitute for Pearson&#39;s correlation coefficient. In contrast to <a
        href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson&#39;s correlation coefficient</a>,
      it is also appropriate for both continuous and discrete variables. Entropy and MI are closely connected concepts.
      As a result, this blog will provide a summary of entropy and discuss how the two are related.</p>
    <h3 id="whats_pointwise_mutual_information"><a href="#whats_pointwise_mutual_information"
        class="header-anchor">What’s Pointwise Mutual Information? </a></h3>
    <p>Pointwise mutual information &#40;PMI&#41;, sometimes known as point mutual information, is a measure of
      association used in statistics, probability theory, and information theory. It contrasts the likelihood of two
      events happening simultaneously with the likelihood of the same events occurring independently.</p>
    <p>PMI, particularly in its positive pointwise mutual information variant, has been referred to as <code>one of the
        most important concepts in NLP</code> sbecause it draws on the intuition that</p>
    <blockquote>
      <p>the best way to weigh the association between two words is to ask how much more the two words co-occur in a
        corpus than we would have expected them to appear by chance.</p>
    </blockquote>
    <p>Robert Fano first proposed the idea in 1961 under the label &quot;mutual information,&quot; however the phrase is
      now more commonly used to refer to a related measure of reliance between random variables: The average PMI of all
      potential events is referred to as the mutual information &#40;MI&#41; of two discrete random variables.</p>
    <h4 id="why_is_it_important_to_our_discussion"><a href="#why_is_it_important_to_our_discussion"
        class="header-anchor">Why Is It Important To Our Discussion? </a></h4>
    <p>When discussing information theory in relation to language processing. It is a method of quantifying
      communication or message transmission through mathematics. Associations between messages are therefore crucial.
    </p>
    <h4 id="mutual_information_and_pointwise_mutual_information"><a
        href="#mutual_information_and_pointwise_mutual_information" class="header-anchor">Mutual Information and
        Pointwise Mutual Information </a></h4>
    <p>Mutual information is theoretically plagued by two issues: In contrast to conventional assessment measures that
      do not differentiate between long and short texts, it assumes independent word variables and gives longer
      documents more weights in the estimate of the feature scores. </p>
    <p>a different version of mutual information is provided that gets over both issues: Weighted Average Pointwise
      Mutual Information &#40;WAPMI&#41;. We offer both strong theoretical and empirical support for WAPMI. </p>
    <p>Additionally, it&#39;s demonstrated that WAPMI possesses a useful quality that other feature metrics do not, more
      specifically <em>the ability to automatically choose the appropriate feature set size by maximizing an objective
        function</em>. This can be accomplished using a straightforward heuristic rather than requiring expensive
      techniques like EM and model selection.</p>
    <h4 id="the_formula_of_mi"><a href="#the_formula_of_mi" class="header-anchor">The Formula of MI</a></h4>
    \[I(W:C) = \sum_{t=1}^{|V|} \sum_{j=1}^{|C|} \text{p}(\text{w}_{t}, \text{c}_{j}) \text{log}
    \frac{\text{p}(\text{w}_{t}|\text{c}_{j})} {\text{p} (\text{w}_{t})}\]
    <p>This could be written as a weighted sum of Kullback-Leibler or KL divergences, because this is the measure of
      information gain between two probability disitributions. <em>p</em> and <em>q</em> is defined as \(D(p || q) =
      \sum_{x} p(x) \text{log} \frac{p(x)}{q(x)}\). </p>
    <p>Therefore, this can be written as the weighted average KL-divergence between the class-conditional distribution
      of words and the global &#40;unconditioned&#41; distribution in the entire corpus: </p>
    \[I(W:C) = \sum_{j=1}^{|C|} \text{p}({c}_{j}) \text{D} (\text{p}(\text{W}|\text{c}_{j})|| {\text{p} (\text{W})})\]
    <p>Now we just need a binary feature to indicate whether the next word in a document is \(w_{t}\), namely \(p(W_{t}
      = 1) = p(W = w_{t})\)</p>
    \[ MI(w_{t}) := I(W_{t}; C) = \sum_{j=1}^{|C|} \sum_{x=0,1} p(W_{t} = x, c_{j}) log \frac{p(W_{t} = x |
    c_{j})}{p(W_{t} = x)} \]
    <p>However, the problem with above formula is that contrary to its assumption of \(w_{t}\) as an independent random
      variable, in fact \(\sum_{t=1}^{|V|} p(W_{t}=1)=1\), so to avoid this problem point-wise mutual information is
      introduced where the formula &#40;2&#41; sums over word scores instead; demonstrated as follows,</p>
    \[PMI(w_{t}) := \sum_{j=1}^{|C|} p(w_{t}, c_{j}) \text{log} \frac{p(w_{t} | c_{t})}{p(w_{t})}\]
    <p>Another problem arises where all training documents in one class is treated according to class-conditional
      probabilities as one big document, so the formula is impacted by individual document length especially the larger
      ones. To resolve this problem, instead using class-conditional distribution, the document-conditional
      probabilities &#40;\(p(w_{t}, c_{j}) = n(w_{t},d_{i})/|d_{i}|\)&#41; are in leu used. Together as a whole, </p>
    \[ WAPMI(w_{t}) := \sum_{j=1}^{|C|} \sum_{d_{i} \in c_{j}} \alpha_{i} p(w_{t} | d_{i}) log
    \frac{p(w_{t}|c_{j})}{p(w_{t})}\]
    <p>where the weight coefficient \(\alpha_{i}\) could be calibrated to account for </p>
    <ul>
      <li>
        <p>\(\alpha_{i} = p(c_{j}) · |d_{i}|/\sum{d_{i} \in c_{j}} |d_{i}|\). This gives each document a weight
          proportional to its lengths. </p>
      </li>
      <li>
        <p>\(\alpha_{i} = 1/ \sum_{j=1}^{|C|} |c_{j}|\). This gives equal weight to all documents. This corresponds to
          an evaluation measure that counts each misclassified document as the same error.</p>
      </li>
      <li>
        <p>\(a_{i} = 1/(|c_{j}| · |C|)\) where \(d_{i} \in c_{j}\). This gives equal weight to the classes by
          normalizing for class size, i.e. documents from small categories receive higher weights.</p>
      </li>
    </ul>
    <p>Here is an example of the <a
        href="https://towardsdatascience.com/multinomial-na&#37;C3&#37;AFve-bayes-classifier-using-pointwise-mutual-information-9ade011fcbd0">Naive
        Bayes Mutual Information Classifier</a></p>
    <div class="page-foot">
      <div class="copyright">
        <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo"
              src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
        ©️ Last modified: January 28, 2024. Website built with <a
          href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia
          programming language</a>.
      </div>
    </div>
  </div><!-- CONTENT ENDS HERE -->

  <script src="/libs/katex/katex.min.js"></script>
  <script src="/libs/katex/contrib/auto-render.min.js"></script>
  <script>
    renderMathInElement(document.body)

  </script>



</body>

</html>
