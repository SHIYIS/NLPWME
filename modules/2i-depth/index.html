<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

  <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">

  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700"
    type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/celeste.min.css">

  <link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
  <!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/apple-touch-icon.png">


  <title>NLPwShiyi - Natural Language Processing with Shiyi</title>
</head>

<body>
  <!-- Latest compiled and minified CSS -->

  <nav id="navbar" class="navigation" role="navigation">
    <input id="toggle1" type="checkbox" />
    <label class="hamburger1" for="toggle1">
      <div class="top"></div>
      <div class="meat"></div>
      <div class="bottom"></div>
    </label>

    <nav class="topnav mx-auto" id="myTopnav">
      <div class="dropdown">
        <button class="dropbtn">Comp Ling
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/1a-info-theory">Information Theory</a>
          <a href="/modules/1b-phil-of-mind">Philosophy of Mind</a>
          <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
          <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
          <a href="/modules/1e-mutual-info">Mutual Information</a>
          <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
          <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
          <a href="/modules/1h-semantics">Logic and Problem Solving</a>
          <a href="/modules/1i-cryptanalysis">Cryptography</a>
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">DL / NLP
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/2a-mdn-nlp">Modern NLP</a>
          <a href="/modules/2b-markov-processes">Markov Processes</a>
          <a href="/modules/2c-word2vec">Word2Vec</a>
          <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
          <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
          <a href="/modules/2f-loss-functions">Stochastic GD</a>
          <a href="/modules/2g-batchnorm">Batchnorm</a>
          <a href="/modules/2h-dropout">Dropout</a>
          <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        </div>
      </div>
      <a href="/" class="active">Intro </a>
      <div class="dropdown">
        <button class="dropbtn">SOTA
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/3a-VAE">Variational Autoencoders</a>
          <a href="/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
          <a href="/modules/3c-transformers">Transformers</a>
          <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">Hands-on
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
          <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
          <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
          <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a>
          <a href="/modules/4e-c4fe-tbip">C4FE-TBIP</a>
          <a href="/modules/4f-etl-job">DE: Serverless ETL</a>

        </div>
      </div>
    </nav>
  </nav>

  <!-- Content appended here -->
  <div class="franklin-content">
    <p><strong>Table of Contents</strong></p>
    <div class="franklin-toc">
      <ol>
        <li><a href="#approximating_an_activation_function">Approximating An Activation Function </a></li>
        <li><a href="#depth_and_related_concepts">Depth and Related Concepts</a></li>
        <li><a href="#the_barron_theorem">The Barron Theorem </a></li>
        <li><a href="#perks_of_depth">Perks of Depth </a></li>
        <li><a href="#problems_with_depth">Problems with Depth</a></li>
      </ol>
    </div>
    <h2 id="approximating_an_activation_function"><a href="#approximating_an_activation_function"
        class="header-anchor">Approximating An Activation Function </a></h2>
    <p>Approximating a certain activation function in the context of neural networks means using an alternative function
      to closely mimic the behavior of the original activation function. Neural networks commonly use activation
      functions to introduce non-linearity into the model, enabling it to learn complex relationships in the data.</p>
    <p>Here are the key points related to approximating activation functions:</p>
    <ol>
      <li>
        <p><strong>Original Activation Function:</strong> Activation functions like sigmoid, hyperbolic tangent
          &#40;tanh&#41;, and rectified linear unit &#40;ReLU&#41; are commonly used in neural networks. Each activation
          function has specific properties and characteristics.</p>
      </li>
      <li>
        <p><strong>Approximation:</strong> In some cases, it might be desirable or necessary to approximate a certain
          activation function with another function that is computationally more efficient, has different
          characteristics, or is more suitable for specific tasks.</p>
      </li>
      <li>
        <p><strong>Function Similarity:</strong> The goal of the approximation is to find another function that behaves
          similarly to the original activation function within a certain range of input values. The approximation should
          capture the key characteristics, such as non-linearity and saturation behavior.</p>
      </li>
      <li>
        <p><strong>Piecewise Linear Approximations:</strong> In some scenarios, piecewise linear functions or step
          functions are used to approximate non-linear activation functions. These approximations can be simpler
          computationally while still introducing the required non-linearity.</p>
      </li>
      <li>
        <p><strong>Benefits of Approximation:</strong></p>
        <ul>
          <li>
            <p><strong>Computational Efficiency:</strong> Some approximations may be computationally less expensive to
              compute than the original activation functions, which can be crucial for large-scale models.</p>
          </li>
          <li>
            <p><strong>Numerical Stability:</strong> Certain functions may be more numerically stable in the training
              process, leading to improved convergence during optimization.</p>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Considerations:</strong> While approximating activation functions can offer benefits, it&#39;s
          essential to carefully consider the impact on model performance, especially if the approximation introduces
          significant deviations from the original function.</p>
      </li>
    </ol>
    <p>For example, a common approximation is using the piecewise linear function ReLU to approximate the sigmoid
      activation function. The ReLU function is computationally more efficient and avoids the vanishing gradient problem
      associated with the sigmoid function.</p>
    <pre><code class="language-python"># Sigmoid function
def sigmoid&#40;x&#41;:
    return 1 / &#40;1 &#43; np.exp&#40;-x&#41;&#41;

# Approximating sigmoid with ReLU
def approx_sigmoid&#40;x&#41;:
    return np.maximum&#40;0, x&#41;</code></pre>
    <p>In practice, the choice of activation function and its approximation depends on the specific requirements of the
      task, the characteristics of the data, and computational considerations.</p>
    <h2 id="depth_and_related_concepts"><a href="#depth_and_related_concepts" class="header-anchor">Depth and Related
        Concepts</a></h2>
    <ul>
      <li>
        <p><strong>Content</strong>: Universal Function Approximation Theorem by Hornik et al. &#40;1991&#41;</p>
      </li>
      <li>
        <p><strong>Summary</strong>: The theorem states that neural networks can approximate any continuous function on
          a compact domain to any degree of accuracy, given sufficient width &#40;number of neurons&#41; in the hidden
          layer. The approximation is within an epsilon &#40;\(\epsilon\)&#41; error margin.</p>
      </li>
    </ul>
    <p>The statement of the Universal Function Approximation Theorem as proposed by Hornik et al. in 1991 is a
      foundational result in the theory of neural networks, stating that a feedforward network with a single hidden
      layer containing a finite number of neurons can approximate continuous functions on compact subsets of
      \(\mathbb{R}^n\), under mild assumptions on the activation function.</p>
    <p>Here is the content of the theorem in LaTeX format:</p>
    <p>Let \(\sigma\) be a nonconstant, bounded, and monotonically-increasing continuous function. For any function \(f
      \in C([0, 1]^d)\) and any \(\varepsilon > 0\), there exists \(h \in \mathbb{N}\) real constants \(v_i, b_i \in
      \mathbb{R}\) and real vectors \(w_i \in \mathbb{R}^d\) such that:</p>
    \[
    \left| \sum_{i=1}^{h} v_i \sigma(w_i^T x + b_i) - f(x) \right| < \varepsilon \] <p>This means that neural networks
      are dense in \(C([0, 1]^d)\), which implies that they can approximate any continuous function on the unit cube in
      \(d\)-dimensional space to any desired degree of accuracy, given sufficient neurons in the hidden layer.</p>
      <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
def relu&#40;x&#41;:
    return np.maximum&#40;x, 0&#41;

def rect&#40;x, a, b, h, eps&#61;1e-7&#41;:
    return h / eps * &#40;
           relu&#40;x - a&#41;
         - relu&#40;x - &#40;a &#43; eps&#41;&#41;
         - relu&#40;x - b&#41;
         &#43; relu&#40;x - &#40;b &#43; eps&#41;&#41;&#41;


x &#61; np.arange&#40;0,5,0.01&#41; # 500
z &#61; np.arange&#40;0,5,0.001&#41;

sin_approx &#61; np.zeros_like&#40;z&#41;
for i in range&#40;2, x.size-1&#41;:
     sin_approx &#61; sin_approx &#43; rect&#40;z,&#40;x&#91;i&#93;&#43;x&#91;i-1&#93;&#41;/2, 
           &#40;x&#91;i&#93;&#43;x&#91;i&#43;1&#93;&#41;/2,  np.sin&#40;x&#91;i&#93;&#41;, 1e-7&#41;
plt.plot&#40;x, y&#41;</code></pre>
      <h2 id="the_barron_theorem"><a href="#the_barron_theorem" class="header-anchor">The Barron Theorem </a></h2>
      <p>The theorem provides a bound on the mean integrated square error between the estimated neural network
        \((\hat{F})\) and the target function \((f)\). The bound is expressed in terms of the number of training points
        &#40;\((N)\)&#41;, the number of neurons &#40;\((q)\)&#41;, the input dimension &#40;\((p)\)&#41;, and a measure
        of the global smoothness of the target function \((\mathcal{C}_f^2)\).</p>
      <p>Here&#39;s a breakdown of the notation and terms in the theorem:</p>
      <ul>
        <li>
          <p>\((\hat{F})\): The estimated neural network or function.</p>
        </li>
        <li>
          <p>\((f)\): The target function that the neural network is trying to approximate.</p>
        </li>
        <li>
          <p>\((\mathcal{C}_f^2)\): The global smoothness of the target function \((f)\).</p>
        </li>
        <li>
          <p>\((N)\): The number of training points or examples.</p>
        </li>
        <li>
          <p>\((q)\): The number of neurons in the neural network.</p>
        </li>
        <li>
          <p>\((p)\): The input dimension.</p>
        </li>
        <li>
          <p>\((O(\cdot))\): The big-O notation, indicating the asymptotic upper bound.</p>
        </li>
      </ul>
      <p>The mean integrated square error is bounded by a term that depends on the smoothness of the target function,
        the number of neurons, the input dimension, and the number of training points.</p>
      <p>The precise form of the bound is given as:</p>
      \[ \mathbb{E}\left[\int (\hat{F}(x) - f(x))^2 dx\right] \leq
      \left(\frac{\mathcal{C}_f^2}{N}\right)O\left(q\mathcal{C}_f^2 + Nqp\log(N)\right) \]
      <p>This bound provides insights into how the mean integrated square error behaves in terms of the complexity of
        the neural network &#40;number of neurons&#41;, the smoothness of the target function, the input dimension, and
        the number of training points. It helps in understanding the trade-offs between these factors in the context of
        the approximation capabilities of neural networks.</p>
      <h2 id="perks_of_depth"><a href="#perks_of_depth" class="header-anchor">Perks of Depth </a></h2>
      <p>In the context of deep learning and neural networks, depth refers to the number of layers in a network. Deeper
        networks have more layers. The benefits of having deep neural networks &#40;high depth&#41; include:</p>
      <ol>
        <li>
          <p><strong>Hierarchy of Features:</strong> Deeper networks can automatically learn hierarchical
            representations of features from the input data. Each layer captures increasingly complex and abstract
            features, enabling the model to understand intricate patterns in the data.</p>
        </li>
        <li>
          <p><strong>Increased Expressiveness:</strong> Depth allows neural networks to represent more complex
            functions. As the depth increases, the network gains the capacity to approximate highly non-linear mappings
            between inputs and outputs.</p>
        </li>
        <li>
          <p><strong>Better Generalization:</strong> Deeper networks tend to generalize well to new, unseen data. They
            can learn more robust and invariant features, reducing the risk of overfitting to the training data.</p>
        </li>
        <li>
          <p><strong>Feature Reusability:</strong> Features learned in early layers of a deep network can be reused
            across different parts of the input space. This enables the model to efficiently capture shared patterns and
            variations in the data.</p>
        </li>
        <li>
          <p><strong>Efficient Parameterization:</strong> Deep architectures enable a more efficient parameterization of
            the model. Instead of requiring an exponentially increasing number of parameters with the input dimension,
            deep networks can capture complex relationships with a manageable number of parameters.</p>
        </li>
        <li>
          <p><strong>Representation Learning:</strong> Deep learning is often associated with representation learning.
            Deeper layers learn useful representations of the input data, which can be valuable for various tasks such
            as image recognition, natural language processing, and speech recognition.</p>
        </li>
        <li>
          <p><strong>Handling Abstractions:</strong> Deep networks excel at learning abstract and high-level
            representations. This makes them well-suited for tasks that involve understanding complex structures or
            relationships in the data.</p>
        </li>
        <li>
          <p><strong>Adaptability:</strong> Deep networks can adapt to different levels of abstraction in the data,
            making them versatile for various applications. They can automatically learn to extract relevant features
            from the raw input.</p>
        </li>
        <li>
          <p><strong>Facilitates Transfer Learning:</strong> The hierarchical nature of deep networks makes them
            suitable for transfer learning. Pre-trained models on large datasets can be fine-tuned for specific tasks
            with smaller datasets, leveraging the learned features.</p>
        </li>
        <li>
          <p><strong>State-of-the-Art Performance:</strong> Many state-of-the-art models across various domains,
            including computer vision, natural language processing, and speech recognition, are deep neural networks.
            The depth of these models contributes to their exceptional performance.</p>
        </li>
      </ol>
      <p>Despite the benefits, it&#39;s important to note that increasing depth also introduces challenges such as
        vanishing/exploding gradients during training and increased computational requirements. Proper architectural
        design, normalization techniques, and regularization methods are often used to address these challenges in deep
        learning.</p>
      <h2 id="problems_with_depth"><a href="#problems_with_depth" class="header-anchor">Problems with Depth</a></h2>
      <p>Depth in neural networks offers several advantages, it also comes with its set of challenges and problems. Some
        of the common problems associated with deep networks include:</p>
      <ol>
        <li>
          <p><strong>Vanishing Gradients:</strong> In deep networks, especially during backpropagation, gradients can
            become very small as they are propagated backward through numerous layers. This can result in slow or
            stalled learning for early layers, making it challenging for them to update their weights effectively.</p>
        </li>
        <li>
          <p><strong>Exploding Gradients:</strong> Conversely, in some cases, gradients can become excessively large
            during backpropagation, leading to numerical instability and making it difficult to optimize the network.
          </p>
        </li>
        <li>
          <p><strong>Computational Complexity:</strong> Deeper networks require more computations during both the
            forward and backward passes. This increased computational complexity can make training and inference more
            resource-intensive.</p>
        </li>
        <li>
          <p><strong>Overfitting:</strong> Deeper networks are prone to overfitting, especially when the amount of
            training data is limited. The model may learn to memorize the training data instead of generalizing well to
            new, unseen data.</p>
        </li>
        <li>
          <p><strong>Difficulty in Training:</strong> Training deep networks can be more challenging due to issues like
            vanishing gradients, and finding an effective set of hyperparameters may require more extensive
            experimentation.</p>
        </li>
        <li>
          <p><strong>Need for More Data:</strong> Deeper networks often require larger amounts of labeled training data
            to generalize well. Insufficient data may lead to poor performance or overfitting.</p>
        </li>
        <li>
          <p><strong>Hyperparameter Tuning:</strong> The presence of more hyperparameters, such as the learning rate,
            weight initialization, and regularization terms, makes hyperparameter tuning more complex and
            time-consuming.</p>
        </li>
        <li>
          <p><strong>Interpretability:</strong> Deeper networks are generally more complex and harder to interpret.
            Understanding the inner workings of deep models and interpreting the learned features can be challenging,
            limiting their explainability.</p>
        </li>
        <li>
          <p><strong>Training Time:</strong> Training deep networks can take a significant amount of time, especially on
            large datasets. This can be a practical concern in scenarios where quick model deployment is essential.</p>
        </li>
        <li>
          <p><strong>Data Dependency:</strong> The effectiveness of deep learning models is often dependent on having
            large amounts of diverse and representative data. In the absence of sufficient data, the benefits of depth
            may not be fully realized.</p>
        </li>
      </ol>
      <p>Researchers and practitioners have developed various techniques to address these challenges, including the use
        of skip connections &#40;e.g., in Residual Networks&#41;, normalization techniques &#40;e.g., Batch
        Normalization&#41;, and careful weight initialization strategies. Despite these challenges, deep learning has
        seen remarkable success in various domains, and ongoing research aims to overcome these limitations and further
        enhance the capabilities of deep networks.</p>
      <div class="page-foot">
        <div class="copyright">
          <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo"
                src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
          ©️ Last modified: March 05, 2024. Website built with <a
            href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia
            programming language</a>.
        </div>
      </div>
  </div><!-- CONTENT ENDS HERE -->

  <script src="/libs/katex/katex.min.js"></script>
  <script src="/libs/katex/contrib/auto-render.min.js"></script>
  <script>
    renderMathInElement(document.body)

  </script>



  <script src="/libs/highlight/highlight.min.js"></script>
  <script>
    hljs.highlightAll();
    hljs.configure({
      tabReplace: '    '
    });

  </script>


</body>

</html>
