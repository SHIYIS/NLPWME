<!doctype html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">


  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,400italic|Source+Code+Pro:400,700"
    type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/celeste.min.css">

  <link rel="icon" type="image/png" sizes="200x200" href="/assets/robot.png">
  <!-- <link rel="shortcut icon" href="/assets/favicon.ico"> -->
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/apple-touch-icon.png">


  <title>NLPwShiyi - Natural Language Processing with Shiyi</title>
</head>

<body>
  <!-- Latest compiled and minified CSS -->

  <nav id="navbar" class="navigation" role="navigation">
    <input id="toggle1" type="checkbox" />
    <label class="hamburger1" for="toggle1">
      <div class="top"></div>
      <div class="meat"></div>
      <div class="bottom"></div>
    </label>

    <nav class="topnav mx-auto" id="myTopnav">
      <div class="dropdown">
        <button class="dropbtn">Comp Ling
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/1a-info-theory">Information Theory</a>
          <a href="/modules/1b-phil-of-mind">Philosophy of Mind</a>
          <a href="/modules/1c-noisy-channel-model">The Noisy Channel Model</a>
          <a href="/modules/1d-finite-automata">FSAs and FSTs</a>
          <a href="/modules/1e-mutual-info">Mutual Information</a>
          <a href="/modules/1f-cky-algorithm">CKY Algorithm</a>
          <a href="/modules/1g-viterbi">Viterbi Algorithm</a>
          <a href="/modules/1h-semantics">Logic and Problem Solving</a>
          <a href="/modules/1i-cryptanalysis">Cryptography</a>
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">DL / NLP
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/2a-mdn-nlp">Modern NLP</a>
          <a href="/modules/2b-markov-processes">Markov Processes</a>
          <a href="/modules/2c-word2vec">Word2Vec</a>
          <a href="/modules/2d-automatic-differentiation">Automatic Differentiation</a>
          <a href="/modules/2e-jax">Jacobian Matrices Derivation</a>
          <a href="/modules/2f-loss-functions">Stochastic GD</a>
          <a href="/modules/2g-batchnorm">Batchnorm</a>
          <a href="/modules/2h-dropout">Dropout</a>
          <a href="/modules/2i-depth">Depth: Pros and Cons</a>
        </div>
      </div>
      <a href="/" class="active">Intro </a>
      <div class="dropdown">
        <button class="dropbtn">SOTA
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/3a-VAE">Variational Autoencoders</a>
          <a href="/modules/3b-dnns">DNNs(RNNs, CNNs, (Bi)-LSTM)</a>
          <a href="/modules/3c-transformers">Transformers</a>
          <!-- <a href="/modules/">TODO: Diffusion Prob Models</a> -->
        </div>
      </div>
      <div class="dropdown">
        <button class="dropbtn">Hands-on
          <i class="fa fa-caret-down"></i>
        </button>
        <div class="dropdown-content">
          <a href="/modules/4a-mlp-from-scratch">MLP from Scratch</a>
          <a href="/modules/4b-generative-adversarial-networks">GAN Example </a>
          <a href="/modules/4c-vae-mnist">VAE for MNIST</a>
          <a href="/modules/4d-bi-lstm-crf">BI-LSTM-CRF S2S</a>
          <a href="/modules/4e-c4fe-tbip">C4FE-TBIP</a>
          <a href="/modules/4f-etl-job">DE: Serverless ETL</a>

        </div>
      </div>
    </nav>
  </nav>

  <!-- Content appended here -->
  <div class="franklin-content">
    <h1 id="the_importance_of_understanding_context_in_natural_language_processing"><a
        href="#the_importance_of_understanding_context_in_natural_language_processing" class="header-anchor">The
        Importance of Understanding Context in Natural Language Processing</a></h1>
    <p><strong>Table of Contents</strong></p>
    <div class="franklin-toc">
      <ol>
        <li><a href="#references">References</a></li>
        <li><a href="#what_is_word2vec">What Is Word2Vec?</a></li>
        <li><a href="#skip-gram_model">Skip-gram Model</a></li>
        <li><a href="#cbow_or_continuous_bag_of_words_model">CBOW or Continuous Bag of Words Model</a></li>
        <li><a href="#a_comparison_between_these_two_model">A Comparison Between These Two Model </a></li>
      </ol>
    </div>
    <p>The old school of language theories before the whole Connectionism came out was boggled by the difficulty of
      incorporating context and nuances. Again as we have talked in the previous sections on <a
        href="https://shiyis.github.io/nlpwme/modules/1-phil-of-mind/">the historical development of NLP</a>,
      Connectionism emerged as a response to perceived limitations in the traditional computational theory of mind,
      particularly in its ability to handle context and capture the complexity of cognitive processes. The computational
      theory of mind, influenced by classical artificial intelligence &#40;AI&#41;, often relied on symbolic
      representations and rule-based systems.</p>
    <p>In the subsequent section of this blog, how the later researches in AI have been able to have a break through in
      terms of solving the problem will be explained.</p>
    <h3 id="references"><a href="#references" class="header-anchor">References</a></h3>
    <p><a href="https://arxiv.org/abs/1402.3722">Word2vec Explained: deriving Mikolov et al.&#39;s negative-sampling
        word-embedding method</a> by Yoav Goldberg and Omer Levy</p>
    <h3 id="what_is_word2vec"><a href="#what_is_word2vec" class="header-anchor">What Is Word2Vec?</a></h3>
    <p>As per definition, Word2Vec is a technique in natural language processing &#40;NLP&#41; that represents words as
      vectors in a continuous vector space, capturing semantic relationships between words. As an alternative to the
      simpler one hot encoding method, one of the reasons was that it could not accurately capture the similarity
      between different words as the cosine similarity could. </p>
    <p>One tool to address the aforementioned issue is Word2vec. It uses a fixed-length vector to represent each word
      and makes use of these vectors to more clearly show the linkages of analogies and similarity between various
      words. The Word2vec tool has two models: the continuous bag of words <a
        href="effective estimate of word representations in vector space">CBOW</a> and the skip-gram &#91;distributed
      representations of words and phrases and their compositionality&#93;. We will next examine the two models and how
      they were trained.</p>
    <p>For the vectors \(\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^d\), the math formula of cosine similarity is
      below, </p>
    \[\frac{\boldsymbol{x}^\top \boldsymbol{y}}{\|\boldsymbol{x}\| \|\boldsymbol{y}\|} \in [-1, 1].\]
    <p>As we have talked about in a different blog, recent researches strive to represent mental processes in a more
      distributed way and also be able to capture their compositionality; therefore, two different methods were
      introduced: Skip-gram and CBOW. The first one is to capture the context within a sentence, and the second one
      smoothly samples words through an algorithm called sliding window, which we will go into details in subsequent
      sections. </p>
    <h3 id="skip-gram_model"><a href="#skip-gram_model" class="header-anchor">Skip-gram Model</a></h3>
    <p>The Skip-gram model in Word2Vec is a type of word embedding model designed to learn distributed representations
      &#40;embeddings&#41; for words in a continuous vector space. The primary objective of Skip-gram is to predict the
      context words given a target word. Let&#39;s break down the key components of the Skip-gram model:</p>
    <blockquote>
      <p><strong>Objective Function</strong>: The training objective is to maximize the probability of the context words
        given the target word. Mathematically, it involves maximizing the conditional probability of the context words
        given the target word.</p>
    </blockquote>
    <blockquote>
      <p><strong>Input-Output Pairs</strong>: For each occurrence of a word in the training data, the Skip-gram model
        generates multiple training examples. Each training example consists of a target word and one of its context
        words. The context words are sampled from a fixed-size window around the target word.</p>
    </blockquote>
    <blockquote>
      <p><strong>Architecture</strong>: The model is typically a neural network with a single hidden layer. The input
        layer represents the target word, and the output layer represents the context words. The hidden layer contains
        the word embeddings &#40;continuous vector representations&#41; for each word in the vocabulary.</p>
    </blockquote>
    <blockquote>
      <p><strong>Softmax Activation</strong>: The output layer uses a softmax activation function, which converts the
        raw output scores into probabilities. These probabilities represent the likelihood of each word being a context
        word given the target word.</p>
    </blockquote>
    <blockquote>
      <p><strong>Training</strong>: During training, the model adjusts its parameters &#40;word embeddings and
        weights&#41; to improve the prediction accuracy of context words for each target word. The training process
        involves backpropagation and gradient descent to minimize the negative log-likelihood of the observed context
        words.</p>
    </blockquote>
    <blockquote>
      <p><strong>Word Embeddings</strong>: Once trained, the hidden layer&#39;s weights serve as the word embeddings.
        These embeddings capture semantic relationships between words based on their co-occurrence patterns.</p>
    </blockquote>
    <blockquote>
      <p><strong>Applications</strong>: The learned word embeddings can be used for various natural language processing
        tasks, such as similarity analysis, language modeling, and as input representations for downstream machine
        learning tasks.</p>
    </blockquote>
    <p>In summary, the Skip-gram model learns word embeddings by training on the task of predicting context words given
      a target word. It captures the distributional semantics of words, representing them as vectors in a continuous
      vector space.</p>
    <h3 id="cbow_or_continuous_bag_of_words_model"><a href="#cbow_or_continuous_bag_of_words_model"
        class="header-anchor">CBOW or Continuous Bag of Words Model</a></h3>
    <p>Continuous Bag of Words &#40;CBOW&#41; model in Word2Vec involves a <code>sliding window</code> during its
      training process. The sliding window is a mechanism used to define the context of a target word.</p>
    <p>Here&#39;s how it typically works:</p>
    <blockquote>
      <p><strong>Context Window</strong>: CBOW considers a fixed-size context window around each target word. This
        window defines the neighboring words that are used as input to predict the target word.</p>
    </blockquote>
    <blockquote>
      <p><strong>Sliding Window</strong>: The sliding window moves through the training text, and at each position, it
        considers the words within the window as the context for the current target word.</p>
    </blockquote>
    <blockquote>
      <p><strong>Training Examples</strong>: For each target word in the text, the CBOW model is trained to predict the
        target word based on the words within its context window.</p>
    </blockquote>
    <blockquote>
      <p><strong>Parameter</strong>: The size of the context window is a parameter that can be set during the training
        process. It determines how many words on either side of the target word are considered as context.</p>
    </blockquote>
    <p>For example, if the context window size is set to 5, the CBOW model will use the five words to the left and five
      words to the right of the target word as the context for training at each step. </p>
    <p>This sliding window mechanism allows the model to capture the local context and syntactic information around each
      word, helping to learn meaningful word embeddings based on the words that tend to co-occur.</p>
    <p>The skip-gram model assumes that a word can be used to generate the words that surround it in a text sequence.
      For example, we assume that the text sequence is <code>the</code>, <code>man</code>, <code>loves</code>,
      <code>his</code>, and <code>son</code>. We use <code>loves</code> as the central target word and set the context
      window size to 2. As shown below, given the central target word <code>loves</code>, the skip-gram model is
      concerned with the conditional probability for generating the context words, <code>the</code>, <code>man</code>,
      <code>his</code> and <code>son</code>, that are within a distance of no more than 2 words, which is,</p>
    \[\mathbb{P}(\textrm{the},\textrm{man},\textrm{his},\textrm{son}\mid\textrm{loves}).\]
    <p>We assume that, given the central target word, the context words are generated independently of each other. In
      this case, the formula above can be rewritten as</p>
    \[\mathbb{P}(\textrm{the}\mid\textrm{loves})\cdot\mathbb{P}(\textrm{man}\mid\textrm{loves})\cdot\mathbb{P}(\textrm{his}\mid\textrm{loves})\cdot\mathbb{P}(\textrm{son}\mid\textrm{loves})
    \]
    <hr />
    <p><img src="https://www.di.ens.fr/~lelarge/skip-gram.svg" alt="skip-gram-demo" /></p>
    <hr />
    <p>In the skip-gram model, each word is represented as two \(d\)-dimension vectors, which are used to compute the
      conditional probability. We assume that the word is indexed as \(i\) in the dictionary, its vector is represented
      as \(\boldsymbol{v}_i\in\mathbb{R}^d\) when it is the central target word, and \(\boldsymbol{u}_i\in\mathbb{R}^d\)
      when it is a context word. Let the central target word \(w_c\) and context word \(w_o\) be indexed as \(c\) and
      \(o\) respectively in the dictionary. The conditional probability of generating the context word for the given
      central target word can be obtained by performing a softmax operation on the vector inner product:</p>
    \[\mathbb{P}(w_o \mid w_c) = \frac{\text{exp}(\boldsymbol{u}_o^\top \boldsymbol{v}_c)}{ \sum_{i \in \mathcal{V}}
    \text{exp}(\boldsymbol{u}_i^\top \boldsymbol{v}_c)}\]
    <p>where vocabulary index set \(\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}\). Assume that a text sequence of
      length \(T\) is given, where the word at time step \(t\) is denoted as \(w^{(t)}\). Assume that context words are
      independently generated given center words. When context window size is \(m\), the likelihood function of the
      skip-gram model is the joint probability of generating all the context words given any center word</p>
    \[ \prod_{t=1}^{T} \prod_{-m \leq j \leq m,\ j \neq 0} \mathbb{P}(w^{(t+j)} \mid w^{(t)})\]
    <p>After the training, for any word in the dictionary with index \(i\), we are going to get its two word vector sets
      \(\boldsymbol{v}_i\) and \(\boldsymbol{u}_i\). In applications of natural language processing &#40;NLP&#41;, the
      central target word vector in the skip-gram model is generally used as the representation vector of a word.</p>
    <h3 id="a_comparison_between_these_two_model"><a href="#a_comparison_between_these_two_model"
        class="header-anchor">A Comparison Between These Two Model </a></h3>
    <p>The Skip-gram model in Word2Vec does not use a fixed sliding window in the same way as the Continuous Bag of
      Words &#40;CBOW&#41; model. Instead, the Skip-gram model considers each word-context pair in the training data
      separately.</p>
    <p>Here&#39;s a brief comparison of the two models:</p>
    <p><strong>CBOW</strong>:</p>
    <p>CBOW predicts a target word based on its surrounding context &#40;a fixed-size window of neighboring words&#41;.
      It sums up the embeddings of the context words and uses them to predict the target word. Skip-gram:</p>
    <p><strong>Skip-gram</strong>:</p>
    <p>On the other hand, takes a target word as input and aims to predict the context words within a certain range. It
      treats each word-context pair in the training data as a separate training example. In the Skip-gram model, there
      is no fixed sliding window that moves through the text. Instead, each word is considered in isolation, and the
      model is trained to predict the words that are likely to appear in its context. The context words can be selected
      from a fixed-size window around the target word, but it&#39;s not constrained by a fixed window during the entire
      training process.</p>
    <p>In summary, while <code>CBOW</code> uses a sliding window to define the context for each target word,
      <code>Skip-gram</code> treats each word-context pair independently without a fixed sliding window. Both of them
      are important models for building the distributed and compositional attributes of an utterance. </p>
    <p>For a full implementation of Word2Vec, please check out this <a
        href="https://github.com/dataflowr/notebooks/blob/master/Module8/08_Word2vec_pytorch_empty.ipynb">notebook</a>.
    </p>
    <div class="page-foot">
      <div class="copyright">
        <a href="https://github.com/shiyis/nlpwsys/tree/master"><b> This page is hosted on <img class="github-logo"
              src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></img></b></a></br>
        ©️ Last modified: February 12, 2024. Website built with <a
          href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia
          programming language</a>.
      </div>
    </div>
  </div><!-- CONTENT ENDS HERE -->

  <script src="/libs/katex/katex.min.js"></script>
  <script src="/libs/katex/contrib/auto-render.min.js"></script>
  <script>
    renderMathInElement(document.body)

  </script>



</body>

</html>
